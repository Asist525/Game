from __future__ import annotations

import gymnasium as gym
import kymnasium as kym  # env ë“±ë¡ìš©
from dataclasses import dataclass
from typing import Any, Dict, Tuple, Optional, List

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.distributions import Categorical
import os
import csv
import torch.optim as optim  
# ================================================================
# 0. ê¸°ë³¸ ìƒìˆ˜
# ================================================================

BOARD_W = 600.0
BOARD_H = 600.0

N_STONES = 3
N_OBS = 3  # obstacles ê°œìˆ˜ (env ê¸°ë³¸ì´ 3ê°œ)

# ë””ìŠ¤í¬ë¦¬íŠ¸ ì•¡ì…˜ ì„¤ì •
N_ANGLES = 24   # ê°ë„ 24bins -> 15ë„ ê°„ê²©
N_POWERS = 6    # íŒŒì›Œ 6ë‹¨ê³„
N_ACTIONS = N_STONES * N_ANGLES * N_POWERS

# ì—°ì† íŒŒì›Œ/ê°ë„ ë²”ìœ„ (env ìŠ¤í™)
MIN_POWER = 1.0
MAX_POWER = 2500.0
ANGLE_LOW = -180.0
ANGLE_HIGH = 180.0

STATE_DIM = 31  # encode_state_basic ì¶œë ¥ ì°¨ì›


# ================================================================
# 1. ê´€ì¸¡/ì•¡ì…˜ íƒ€ì… ì •ì˜
# ================================================================

@dataclass
class AlkkagiAction:
    """ì•Œê¹Œê¸° í™˜ê²½ì— ë„˜ê¸¸ ì•¡ì…˜ dictë¥¼ íƒ€ì… ì•ˆì „í•˜ê²Œ ë‹¤ë£¨ê¸° ìœ„í•œ êµ¬ì¡°ì²´."""
    turn: int      # 0: black, 1: white
    index: int     # ëŒ ì¸ë±ìŠ¤ (0,1,2)
    power: float   # [1, 2500]
    angle: float   # [-180, 180]


@dataclass
class AlkkagiObservation:
    """ì›ì‹œ obs dictë¥¼ ì¡°ê¸ˆ ë” íƒ€ì… ë¶„ëª…í•˜ê²Œ ê°ì‹¸ëŠ” ë˜í¼."""
    raw: Dict[str, Any]

    @property
    def turn(self) -> int:
        # 0: í‘ ì°¨ë¡€, 1: ë°± ì°¨ë¡€
        return int(self.raw["turn"])

    @property
    def black(self) -> np.ndarray:
        # shape: (3, 3) -> [ [x, y, alive], ... ]
        return np.array(self.raw["black"], dtype=np.float32)

    @property
    def white(self) -> np.ndarray:
        return np.array(self.raw["white"], dtype=np.float32)

    @property
    def obstacles(self) -> np.ndarray:
        # shape: (3, 4) -> [x, y, w, h]
        return np.array(self.raw["obstacles"], dtype=np.float32)


# ================================================================
# 2. í™˜ê²½ ë˜í¼ + ë¦¬ì›Œë“œ ì…°ì´í•‘
# ================================================================

# ë¦¬ì›Œë“œ ì…°ì´í•‘ ìƒìˆ˜
WIN_REWARD       = 5.0    # ë˜ëŠ” 5.0
KILL_BONUS       = 1.0    # ìƒëŒ€ ëŒ í•œ ê°œ â†’ +1
SELF_LOSS_PENAL  = 0    # ë‚´ ëŒ í•œ ê°œ â†’ -1 => ì¼ë‹¨ ì—†ì• ê¸°
STEP_PENALTY     = 0.001   # ê·¸ëŒ€ë¡œ ë‘ê±°ë‚˜, ì²´ê°ë˜ê²Œ 0.001 ì •ë„(ì‹¤ì œë¡œëŠ” -0.001)



def count_alive_stones(obs: AlkkagiObservation, color: int) -> int:
    """color: 0=black, 1=white ê¸°ì¤€ìœ¼ë¡œ ì‚´ì•„ìˆëŠ” ëŒ ê°œìˆ˜."""
    assert color in (0, 1)
    stones = obs.black if color == 0 else obs.white
    return int((stones[:, 2] > 0.5).sum())


def compute_shaped_reward(
    prev_obs: AlkkagiObservation,
    next_obs: AlkkagiObservation,
    acting_color: int,
    terminated: bool,
    truncated: bool,
) -> float:
    """
    í•œ ìŠ¤í… ë™ì•ˆì˜ shaped rewardë¥¼ ê³„ì‚°.
    - acting_color: ì´ë²ˆ ìŠ¤í…ì—ì„œ í–‰ë™í•œ ìª½(0=í‘, 1=ë°±)
    """
    assert acting_color in (0, 1)

    my_color  = acting_color
    opp_color = 1 - acting_color

    my_alive_prev  = count_alive_stones(prev_obs, my_color)
    opp_alive_prev = count_alive_stones(prev_obs, opp_color)
    my_alive_next  = count_alive_stones(next_obs, my_color)
    opp_alive_next = count_alive_stones(next_obs, opp_color)

    kill_diff      = opp_alive_prev - opp_alive_next   # ë‚´ê°€ ì£½ì¸ ìƒëŒ€ ëŒ ìˆ˜
    self_loss_diff = my_alive_prev - my_alive_next     # ë‚´ê°€ ìƒì€ ë‚´ ëŒ ìˆ˜

    reward = 0.0
    reward += KILL_BONUS      * kill_diff
    reward -= SELF_LOSS_PENAL * self_loss_diff
    reward -= STEP_PENALTY

    # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ ìŠ¹/íŒ¨ ë³´ë„ˆìŠ¤
    if terminated or truncated:
        if opp_alive_next == 0 and my_alive_next > 0:
            # ë‚´ê°€ ì´ê¹€
            reward += WIN_REWARD
        elif my_alive_next == 0 and opp_alive_next > 0:
            # ë‚´ê°€ ì§
            reward -= WIN_REWARD
        # ë‘˜ ë‹¤ 0 ë˜ëŠ” ë‘˜ ë‹¤ >0 ì´ë©´ ë¬´ìŠ¹ë¶€ -> ì¶”ê°€ ë³´ìƒ ì—†ìŒ

    return float(reward)


class AlkkagiEnvWrapper:
    """
    kymnasium/AlKkaGi-3x3-v0 ë˜í¼.
    - reset / step ì¸í„°í˜ì´ìŠ¤ ì •ë¦¬
    - reward ì…°ì´í•‘ ì ìš©
    """
    def __init__(
        self,
        render_mode: Optional[str] = None,  # "human" / "rgb_array" / None
        bgm: bool = False,
    ):
        self.env = gym.make(
            "kymnasium/AlKkaGi-3x3-v0",
            obs_type="custom",
            render_mode=render_mode,
            bgm=bgm,
        )
        self.last_obs: Optional[AlkkagiObservation] = None
        self.last_info: Optional[Dict[str, Any]] = None

    def reset(
        self,
        seed: Optional[int] = None,
    ) -> Tuple[AlkkagiObservation, Dict[str, Any]]:
        obs_raw, info = self.env.reset(seed=seed)
        obs = AlkkagiObservation(obs_raw)
        self.last_obs = obs
        self.last_info = info
        return obs, info

    def step(
        self,
        action: AlkkagiAction,
    ) -> Tuple[AlkkagiObservation, float, bool, bool, Dict[str, Any]]:
        act_dict = {
            "turn": int(action.turn),
            "index": int(action.index),
            "power": float(action.power),
            "angle": float(action.angle),
        }

        prev_obs = self.last_obs
        obs_raw, env_reward, terminated, truncated, info = self.env.step(act_dict)
        next_obs = AlkkagiObservation(obs_raw)

        if prev_obs is None:
            shaped_reward = 0.0
        else:
            shaped_reward = compute_shaped_reward(
                prev_obs=prev_obs,
                next_obs=next_obs,
                acting_color=action.turn,
                terminated=terminated,
                truncated=truncated,
            )

        self.last_obs = next_obs
        self.last_info = info

        return next_obs, shaped_reward, bool(terminated), bool(truncated), info

    def close(self):
        self.env.close()


# ================================================================
# 3. ê´€ì¸¡ ì¸ì½”ë” (31ì°¨ì›)
# ================================================================

def encode_state_basic(
    obs: AlkkagiObservation,
    my_color: int,
) -> np.ndarray:
    """
    ì•Œê¹Œê¸° ê´€ì¸¡ì„ 31ì°¨ì› ì‹¤ìˆ˜ ë²¡í„°ë¡œ ì¸ì½”ë”©.
    - í•­ìƒ 'ë‚´ ê´€ì (me vs opp)'ìœ¼ë¡œ ì •ë ¬
    - ì¢Œí‘œ/í¬ê¸°ëŠ” [0,1] ë²”ìœ„ë¡œ ì •ê·œí™”
    """
    assert my_color in (0, 1), "my_colorëŠ” 0(í‘) ë˜ëŠ” 1(ë°±)ì´ì–´ì•¼ í•©ë‹ˆë‹¤."

    # í˜„ì¬ í„´ ì •ë³´
    is_my_turn = 1.0 if obs.turn == my_color else 0.0

    # ë‚´ ëŒ / ìƒëŒ€ ëŒ ë¶„ë¦¬
    if my_color == 0:  # ë‚´ê°€ í‘
        me = obs.black   # shape (3,3)
        opp = obs.white
    else:               # ë‚´ê°€ ë°±
        me = obs.white
        opp = obs.black

    obstacles = obs.obstacles  # shape (3,4)

    features = np.zeros(STATE_DIM, dtype=np.float32)
    idx = 0

    # 1) ë‚´ í„´ì¸ì§€ ì—¬ë¶€
    features[idx] = is_my_turn
    idx += 1

    # 2) ë‚˜(me)ì˜ ëŒ 3ê°œ: ê° [x_norm, y_norm, alive]
    for i in range(N_STONES):
        x, y, alive = me[i]
        features[idx + 0] = x / BOARD_W
        features[idx + 1] = y / BOARD_H
        features[idx + 2] = alive
        idx += 3

    # 3) ìƒëŒ€(opp)ì˜ ëŒ 3ê°œ
    for i in range(N_STONES):
        x, y, alive = opp[i]
        features[idx + 0] = x / BOARD_W
        features[idx + 1] = y / BOARD_H
        features[idx + 2] = alive
        idx += 3

    # 4) ì¥ì• ë¬¼ 3ê°œ: ê° [x_norm, y_norm, w_norm, h_norm]
    for i in range(N_OBS):
        x, y, w, h = obstacles[i]
        features[idx + 0] = x / BOARD_W
        features[idx + 1] = y / BOARD_H
        features[idx + 2] = w / BOARD_W
        features[idx + 3] = h / BOARD_H
        idx += 4

    assert idx == STATE_DIM, f"feature index mismatch: idx={idx}"
    return features


def encode_state_basic_tensor(
    obs: AlkkagiObservation,
    my_color: int,
    device: torch.device | str = "cpu",
) -> torch.Tensor:
    feat_np = encode_state_basic(obs, my_color)
    return torch.from_numpy(feat_np).to(device=device, dtype=torch.float32)


# ================================================================
# 4. ë””ìŠ¤í¬ë¦¬íŠ¸ ì•¡ì…˜ + ë§ˆìŠ¤í¬
# ================================================================

def decode_discrete_action(action_idx: int) -> tuple[int, int, int]:
    """
    ë””ìŠ¤í¬ë¦¬íŠ¸ ì•¡ì…˜ ì¸ë±ìŠ¤ -> (stone_idx, angle_idx, power_idx)
    """
    assert 0 <= action_idx < N_ACTIONS, f"action_idx ë²”ìœ„ ì˜¤ë¥˜: {action_idx}"

    stones_block = N_ANGLES * N_POWERS
    stone_idx = action_idx // stones_block
    rem = action_idx % stones_block
    angle_idx = rem // N_POWERS
    power_idx = rem % N_POWERS
    return stone_idx, angle_idx, power_idx


def bins_to_angle(angle_idx: int) -> float:
    """
    angle_idx (0..N_ANGLES-1) -> ì‹¤ì œ ê°ë„ ê°’ (ë„ ë‹¨ìœ„)
    """
    assert 0 <= angle_idx < N_ANGLES
    delta = (ANGLE_HIGH - ANGLE_LOW) / N_ANGLES  # 360 / N_ANGLES
    angle = ANGLE_LOW + (angle_idx + 0.5) * delta
    return float(angle)


def bins_to_power(power_idx: int) -> float:
    """
    power_idx (0..N_POWERS-1) -> ì‹¤ì œ íŒŒì›Œ ê°’
    """
    assert 0 <= power_idx < N_POWERS
    delta = (MAX_POWER - MIN_POWER) / N_POWERS
    power = MIN_POWER + (power_idx + 0.5) * delta
    return float(power)


def discrete_to_env_action(
    action_idx: int,
    obs: AlkkagiObservation,
) -> AlkkagiAction:
    """
    ë””ìŠ¤í¬ë¦¬íŠ¸ ì•¡ì…˜ ì¸ë±ìŠ¤ë¥¼ envì— ë„£ì„ ìˆ˜ ìˆëŠ” AlkkagiActionìœ¼ë¡œ ë³€í™˜.
    - turnì€ í•­ìƒ obs.turnì— ë§ì¶˜ë‹¤ (ì§€ê¸ˆ ëˆ„êµ¬ ì°¨ë¡€ì¸ì§€).
    """
    stone_idx, angle_idx, power_idx = decode_discrete_action(action_idx)

    angle = bins_to_angle(angle_idx)
    power = bins_to_power(power_idx)
    turn = obs.turn

    return AlkkagiAction(
        turn=turn,
        index=stone_idx,
        power=power,
        angle=angle,
    )


def get_valid_action_mask(obs: AlkkagiObservation) -> np.ndarray:
    """
    í˜„ì¬ obs ê¸°ì¤€ìœ¼ë¡œ ìœ íš¨í•œ ë””ìŠ¤í¬ë¦¬íŠ¸ ì•¡ì…˜ ë§ˆìŠ¤í¬ ë°˜í™˜.
    - ê¸°ì¤€: í˜„ì¬ í„´(obs.turn)ì˜ ì‚´ì•„ìˆëŠ” ëŒë§Œ ì„ íƒ ê°€ëŠ¥
    - ë°˜í™˜: shape (N_ACTIONS,), ê°’ì€ {0.0, 1.0}
    """
    my_color = obs.turn
    stones = obs.black if my_color == 0 else obs.white
    alive_indices = [i for i, s in enumerate(stones) if s[2] > 0.5]

    mask = np.zeros(N_ACTIONS, dtype=np.float32)

    for a in range(N_ACTIONS):
        stone_idx, _, _ = decode_discrete_action(a)
        if stone_idx in alive_indices:
            mask[a] = 1.0

    # ì „ë¶€ 0ì´ë©´ fallback
    if mask.sum() == 0:
        mask[:] = 1.0

    return mask


# ================================================================
# 5. Policy + Value ë„¤íŠ¸ì›Œí¬
# ================================================================

class PolicyValueNet(nn.Module):
    """
    ì•Œê¹Œê¸°ìš© ì •ì±…+ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬.
    - ì…ë ¥: 31ì°¨ì› ìƒíƒœ (ë‚´ ê´€ì )
    - ì¶œë ¥: policy_logits (N_ACTIONS), state_value (1,)
    """
    def __init__(self, state_dim: int = STATE_DIM, n_actions: int = N_ACTIONS):
        super().__init__()
        self.state_dim = state_dim
        self.n_actions = n_actions

        hidden_size = 256

        self.fc1 = nn.Linear(state_dim, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)

        self.policy_head = nn.Linear(hidden_size, n_actions)
        self.value_head = nn.Linear(hidden_size, 1)

    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:
        """
        x: (B, state_dim) ë˜ëŠ” (state_dim,)
        return:
          - logits: (B, n_actions)
          - value:  (B,)
        """
        if x.dim() == 1:
            x = x.unsqueeze(0)  # (state_dim,) -> (1, state_dim)

        h = F.relu(self.fc1(x))
        h = F.relu(self.fc2(h))

        logits = self.policy_head(h)              # (B,N_ACTIONS)
        value = self.value_head(h).squeeze(-1)    # (B,)

        return logits, value


# ================================================================
# 6. Rollout Buffer + GAE
# ================================================================

class RolloutBuffer:
    def __init__(self):
        self.states: List[np.ndarray] = []
        self.actions: List[int] = []
        self.log_probs: List[float] = []
        self.rewards: List[float] = []
        self.dones: List[bool] = []
        self.values: List[float] = []
        self.masks: List[np.ndarray] = []  # valid action mask

        self.advantages: Optional[np.ndarray] = None
        self.returns: Optional[np.ndarray] = None

    def clear(self):
        self.states.clear()
        self.actions.clear()
        self.log_probs.clear()
        self.rewards.clear()
        self.dones.clear()
        self.values.clear()
        self.masks.clear()
        self.advantages = None
        self.returns = None

    def add(
        self,
        state_np: np.ndarray,
        action: int,
        log_prob: float,
        reward: float,
        done: bool,
        value: float,
        mask_np: np.ndarray,
    ):
        self.states.append(state_np.astype(np.float32))
        self.actions.append(int(action))
        self.log_probs.append(float(log_prob))
        self.rewards.append(float(reward))
        self.dones.append(bool(done))
        self.values.append(float(value))
        self.masks.append(mask_np.astype(np.float32))

    def compute_returns_and_advantages(
        self,
        gamma: float = 0.99,
        gae_lambda: float = 0.95,
    ):
        T = len(self.rewards)
        self.advantages = np.zeros(T, dtype=np.float32)
        self.returns = np.zeros(T, dtype=np.float32)

        last_adv = 0.0
        next_value = 0.0  # ë§ˆì§€ë§‰ ë‹¤ìŒ ìƒíƒœì˜ V(s')ëŠ” 0ìœ¼ë¡œ

        for t in reversed(range(T)):
            done = self.dones[t]
            mask = 0.0 if done else 1.0

            delta = (
                self.rewards[t]
                + gamma * next_value * mask
                - self.values[t]
            )
            last_adv = delta + gamma * gae_lambda * mask * last_adv

            self.advantages[t] = last_adv
            next_value = self.values[t]

        self.returns = self.advantages + np.array(self.values, dtype=np.float32)


# ================================================================
# 7. PPO ì—…ë°ì´íŠ¸
# ================================================================

def ppo_update(
    net: PolicyValueNet,
    optimizer: torch.optim.Optimizer,
    buffer: RolloutBuffer,
    device: torch.device | str = "cpu",
    batch_size: int = 512,
    ppo_epochs: int = 4,
    clip_coef: float = 0.2,
    value_coef: float = 0.5,
    entropy_coef: float = 0.01,
    max_grad_norm: float = 0.5,
):
    net.train()

    device = torch.device(device)

    # numpy ë¦¬ìŠ¤íŠ¸ë¥¼ ë¨¼ì € ìŠ¤íƒí•´ì„œ í…ì„œë¡œ ë³€í™˜ (ê²½ê³  ì œê±°)
    states_np = np.array(buffer.states, dtype=np.float32)   # (T,31)
    masks_np = np.array(buffer.masks, dtype=np.float32)     # (T,N_ACTIONS)

    states = torch.from_numpy(states_np).to(device)         # (T,31)
    masks = torch.from_numpy(masks_np).to(device)           # (T,N_ACTIONS)

    actions = torch.tensor(buffer.actions, dtype=torch.long, device=device)
    old_log_probs = torch.tensor(buffer.log_probs, dtype=torch.float32, device=device)
    returns = torch.tensor(buffer.returns, dtype=torch.float32, device=device)
    advantages = torch.tensor(buffer.advantages, dtype=torch.float32, device=device)

    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

    T = states.size(0)
    indices = np.arange(T)

    for epoch in range(ppo_epochs):
        np.random.shuffle(indices)

        for start in range(0, T, batch_size):
            end = start + batch_size
            mb_idx = indices[start:end]

            mb_states = states[mb_idx]      # (B,31)
            mb_masks = masks[mb_idx]        # (B,N_ACTIONS)
            mb_actions = actions[mb_idx]    # (B,)
            mb_old_log_probs = old_log_probs[mb_idx]
            mb_returns = returns[mb_idx]
            mb_advantages = advantages[mb_idx]

            logits, values = net(mb_states)  # logits: (B,N_ACTIONS), values: (B,)

            # ìœ íš¨ ì•¡ì…˜ ë§ˆìŠ¤í¬ ì ìš©
            invalid = mb_masks < 0.5
            logits = logits.masked_fill(invalid, -1e9)

            dist = Categorical(logits=logits)
            new_log_probs = dist.log_prob(mb_actions)  # (B,)
            entropy = dist.entropy().mean()

            ratio = (new_log_probs - mb_old_log_probs).exp()
            surr1 = ratio * mb_advantages
            surr2 = torch.clamp(ratio, 1.0 - clip_coef, 1.0 + clip_coef) * mb_advantages
            policy_loss = -torch.min(surr1, surr2).mean()

            value_loss = F.mse_loss(values, mb_returns)

            loss = policy_loss + value_coef * value_loss - entropy_coef * entropy

            optimizer.zero_grad()
            loss.backward()
            nn.utils.clip_grad_norm_(net.parameters(), max_grad_norm)
            optimizer.step()


# ================================================================
# 8. self-play rollout + ì „ì²´ í•™ìŠµ ë£¨í”„
# ================================================================

def collect_rollout(
    envw: AlkkagiEnvWrapper,
    net: PolicyValueNet,
    buffer: RolloutBuffer,
    device: torch.device | str,
    rollout_steps: int,
    gamma: float,
    gae_lambda: float,
) -> tuple[list[float], list[int], int, int, int]:
    """
    ë°˜í™˜:
      - episode_returns: ê° ì—í”¼ì†Œë“œ ì´ ë¦¬í„´ ë¦¬ìŠ¤íŠ¸
      - episode_lengths: ê° ì—í”¼ì†Œë“œ ìŠ¤í… ìˆ˜
      - wins_black: ì´ë²ˆ rolloutì—ì„œ í‘ ìŠ¹ ìˆ˜
      - wins_white: ì´ë²ˆ rolloutì—ì„œ ë°± ìŠ¹ ìˆ˜
      - draws: ë¬´ìŠ¹ë¶€ ìˆ˜
    """
    net.eval()
    device = torch.device(device)

    buffer.clear()
    episode_returns: list[float] = []
    episode_lengths: list[int] = []

    wins_black = 0
    wins_white = 0
    draws = 0

    obs, info = envw.reset()
    ep_return = 0.0
    ep_len = 0

    for step in range(rollout_steps):
        ep_len += 1

        my_color = obs.turn  # í˜„ì¬ í„´ ê¸°ì¤€ ì¸ì½”ë”©

        state_np = encode_state_basic(obs, my_color)  # (31,)
        state_tensor = torch.from_numpy(state_np).to(device=device, dtype=torch.float32)
        state_tensor = state_tensor.unsqueeze(0)      # (1,31)

        mask_np = get_valid_action_mask(obs)          # (N_ACTIONS,)
        mask_tensor = torch.from_numpy(mask_np).to(device=device, dtype=torch.float32)
        mask_tensor = mask_tensor.unsqueeze(0)        # (1,N_ACTIONS)

        with torch.no_grad():
            logits, value = net(state_tensor)         # logits: (1,N_ACTIONS)
            invalid = mask_tensor < 0.5               # (1,N_ACTIONS)
            logits_masked = logits.masked_fill(invalid, -1e9)

            dist = Categorical(logits=logits_masked)
            action_idx_tensor = dist.sample()         # (1,)
            log_prob_tensor = dist.log_prob(action_idx_tensor)

        action_idx = int(action_idx_tensor.item())
        log_prob = float(log_prob_tensor.item())
        value_scalar = float(value.squeeze(0).item())

        env_action = discrete_to_env_action(action_idx, obs)

        next_obs, reward, terminated, truncated, info = envw.step(env_action)
        done = bool(terminated or truncated)

        ep_return += float(reward)

        # ğŸ”» stepë³„ ë¦¬ì›Œë“œ + í˜„ì¬ ì—í”¼ì†Œë“œ ëˆ„ì  ë¦¬í„´ ì¶œë ¥
        print(
            f"[ROLL] step={step:04d}, ep_step={ep_len:03d}, "
            f"reward={reward:.3f}, ep_return={ep_return:.3f}"
        )

        buffer.add(
            state_np=state_np,
            action=action_idx,
            log_prob=log_prob,
            reward=float(reward),
            done=done,
            value=value_scalar,
            mask_np=mask_np,
        )

        if done:
            # ì—í”¼ì†Œë“œ ì¢…ë£Œ: ë¦¬í„´/ê¸¸ì´ ê¸°ë¡
            episode_returns.append(ep_return)
            episode_lengths.append(ep_len)

            black_alive = count_alive_stones(next_obs, 0)
            white_alive = count_alive_stones(next_obs, 1)

            # ğŸ”» ì—í”¼ì†Œë“œ ìš”ì•½ ë¡œê·¸
            print(
                f"[EP DONE] ep_return={ep_return:.3f}, ep_steps={ep_len}, "
                f"black_alive={black_alive}, white_alive={white_alive}"
            )

            # ìŠ¹/íŒ¨/ë¬´ íŒì • (ë§ˆì§€ë§‰ next_obs ê¸°ì¤€)
            if black_alive > 0 and white_alive == 0:
                wins_black += 1
            elif white_alive > 0 and black_alive == 0:
                wins_white += 1
            else:
                draws += 1

            ep_return = 0.0
            ep_len = 0
            obs, info = envw.reset()
        else:
            obs = next_obs

    buffer.compute_returns_and_advantages(gamma=gamma, gae_lambda=gae_lambda)
    return episode_returns, episode_lengths, wins_black, wins_white, draws


def train_ppo_selfplay(
    num_updates: int = 1500,
    rollout_steps: int = 2048,
    gamma: float = 0.99,
    gae_lambda: float = 0.95,
    lr: float = 3e-4,
    batch_size: int = 512,
    ppo_epochs: int = 4,
    clip_coef: float = 0.2,
    value_coef: float = 0.5,
    entropy_coef: float = 0.01,
    max_grad_norm: float = 0.5,
    device: torch.device | str = "cpu",
    save_path: str | None = "alkkagi_ppo.pt",
    log_csv_path: str | None = "training_log.csv",
    checkpoint_every_episodes: int = 1000,
    checkpoint_dir: str = "checkpoints",
) -> PolicyValueNet:
    """
    ê°„ë‹¨í•œ single-env self-play PPO í•™ìŠµ ë£¨í”„ + ì²´í¬í¬ì¸íŠ¸/CSV ë¡œê¹….
    """
    device = torch.device(device)

    envw = AlkkagiEnvWrapper(render_mode='human', bgm=False)
    net = PolicyValueNet().to(device)
    optimizer = optim.Adam(net.parameters(), lr=lr)
    buffer = RolloutBuffer()

    total_black = 0
    total_white = 0
    total_draws = 0

    total_episodes = 0
    next_ckpt_ep = checkpoint_every_episodes

    learner_rating = 1500.0
    num_players = 2  # self-play í‘/ë°±

    # CSV í—¤ë” ì´ˆê¸°í™”
    if log_csv_path is not None and (not os.path.exists(log_csv_path)):
        with open(log_csv_path, "w", newline="") as f:
            writer = csv.writer(f)
            writer.writerow([
                "epoch",
                "episodes",
                "wins",
                "draws",
                "losses",
                "win_rate",
                "avg_reward",
                "avg_steps",
                "learner_rating",
                "num_players",
            ])

    for update in range(1, num_updates + 1):
        ep_returns, ep_lengths, wins_black, wins_white, draws = collect_rollout(
            envw=envw,
            net=net,
            buffer=buffer,
            device=device,
            rollout_steps=rollout_steps,
            gamma=gamma,
            gae_lambda=gae_lambda,
        )

        episodes = len(ep_returns)
        total_episodes += episodes

        total_black += wins_black
        total_white += wins_white
        total_draws += draws

        ppo_update(
            net=net,
            optimizer=optimizer,
            buffer=buffer,
            device=device,
            batch_size=batch_size,
            ppo_epochs=ppo_epochs,
            clip_coef=clip_coef,
            value_coef=value_coef,
            entropy_coef=entropy_coef,
            max_grad_norm=max_grad_norm,
        )

        if episodes > 0:
            avg_return = float(np.mean(ep_returns))
            avg_steps = float(np.mean(ep_lengths))
        else:
            avg_return = 0.0
            avg_steps = 0.0

        wins = wins_black
        losses = wins_white

        if episodes > 0:
            win_rate = wins / episodes
        else:
            win_rate = 0.0

        # ê°„ë‹¨í•œ Elo ìŠ¤íƒ€ì¼ rating ì—…ë°ì´íŠ¸ (ìƒëŒ€ë„ 1500 ê°€ì •)
        if wins + losses > 0:
            score = wins / (wins + losses)      # ì‹¤ì œ ì ìˆ˜
            expected = 0.5                      # ë™ê¸‰ì´ë¼ê³  ê°€ì •
            K = 32.0
            learner_rating += K * (score - expected)

        # ì½˜ì†” ë¡œê·¸
        print(
            f"[Update {update:04d}] "
            f"ep_total={total_episodes}, ep_this={episodes}, "
            f"wins={wins}, draws={draws}, losses={losses}, "
            f"win_rate={win_rate:.3f}, "
            f"avg_return={avg_return:.3f}, avg_steps={avg_steps:.2f}, "
            f"rating={learner_rating:.2f}"
        )

        # CSV ë¡œê·¸ ê¸°ë¡
        if log_csv_path is not None:
            with open(log_csv_path, "a", newline="") as f:
                writer = csv.writer(f)
                writer.writerow([
                    update,            # epoch
                    episodes,
                    wins,
                    draws,
                    losses,
                    f"{win_rate:.6f}",
                    f"{avg_return:.6f}",
                    f"{avg_steps:.6f}",
                    f"{learner_rating:.6f}",
                    num_players,
                ])

        # ì—í”¼ì†Œë“œ ê¸°ì¤€ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        if checkpoint_every_episodes > 0:
            while total_episodes >= next_ckpt_ep:
                os.makedirs(checkpoint_dir, exist_ok=True)
                ckpt_path = os.path.join(
                    checkpoint_dir,
                    f"alkkagi_ep_{next_ckpt_ep:06d}.pt",
                )
                torch.save(net.state_dict(), ckpt_path)
                print(f"[CKPT] Saved checkpoint at {ckpt_path} (episodes={next_ckpt_ep})")
                next_ckpt_ep += checkpoint_every_episodes

    envw.close()

    # ìµœì¢… ëª¨ë¸ ì €ì¥
    if save_path is not None:
        torch.save(net.state_dict(), save_path)
        print(f"[SAVE] Saved final model to {save_path}")

    return net


# ================================================================
# 10. ì œì¶œìš© ì—ì´ì „íŠ¸: YourBlackAgent / YourWhiteAgent
# ================================================================
# ìœ„ìª½ì— ì´ë¯¸ ìˆëŠ” importë“¤:
# import kymnasium as kym
# import torch
# from typing import Any, Dict

class YourAlkkagiAgentBase(kym.Agent):
    """
    ëŒ€íšŒ ì œì¶œìš© ê¸°ë³¸ Agent.
    - color: 0(í‘), 1(ë°±)
    - PolicyValueNetì„ ë‚´ë¶€ì— ë“¤ê³  ìˆê³ ,
      obs -> (31ì°¨ì› ì¸ì½”ë”©) -> ë„¤íŠ¸ì›Œí¬ -> ë””ìŠ¤í¬ë¦¬íŠ¸ ì•¡ì…˜ -> env action dict
    """
    def __init__(
        self,
        net: PolicyValueNet,
        color: int,
        device: torch.device | str = "cpu",
    ):
        super().__init__()
        assert color in (0, 1)
        self.color = color
        self.device = torch.device(device)
        self.net = net.to(self.device)
        self.net.eval()

    def act(self, observation: Any, info: Dict) -> Dict[str, float]:
        """
        envê°€ ë§¤ ìŠ¤í…ë§ˆë‹¤ í˜¸ì¶œí•˜ëŠ” í•¨ìˆ˜.
        - observation: envì—ì„œ ë„˜ì–´ì˜¤ëŠ” raw dict
        - ë°˜í™˜: {"turn", "index", "power", "angle"}
        """
        obs = AlkkagiObservation(observation)

        # ë°©ì–´ìš©: ë‚´ ì°¨ë¡€ê°€ ì•„ë‹Œë° í˜¸ì¶œë˜ë©´ ë¬´ì‹œë˜ëŠ” ì•¡ì…˜ ë¦¬í„´
        if obs.turn != self.color:
            return {
                "turn": obs.turn,
                "index": 0,
                "power": 0.0,
                "angle": 0.0,
            }

        my_color = obs.turn  # canonical: í•­ìƒ í˜„ì¬ í„´ ê¸°ì¤€ ì¸ì½”ë”©

        # 1) ìƒíƒœ ì¸ì½”ë”© (31ì°¨ì›)
        state_np = encode_state_basic(obs, my_color)  # (31,)
        state_tensor = (
            torch.from_numpy(state_np)
            .to(self.device, dtype=torch.float32)
            .unsqueeze(0)  # (1,31)
        )

        # 2) ìœ íš¨ ì•¡ì…˜ ë§ˆìŠ¤í¬
        mask_np = get_valid_action_mask(obs)          # (N_ACTIONS,)
        mask_tensor = (
            torch.from_numpy(mask_np)
            .to(self.device, dtype=torch.float32)
            .unsqueeze(0)  # (1, N_ACTIONS)
        )

        # 3) ì •ì±… ë„¤íŠ¸ì›Œí¬ forward + ë§ˆìŠ¤í¬ ì ìš© + argmaxë¡œ ì•¡ì…˜ ì„ íƒ
        with torch.no_grad():
            logits, _ = self.net(state_tensor)        # logits: (1,N_ACTIONS)
            invalid = mask_tensor < 0.5
            logits_masked = logits.masked_fill(invalid, -1e9)

            # í‰ê°€ ì‹œì—ëŠ” deterministicí•˜ê²Œ argmax ì‚¬ìš©
            action_idx_tensor = torch.argmax(logits_masked, dim=-1)  # (1,)
            action_idx = int(action_idx_tensor.item())

        # 4) ë””ìŠ¤í¬ë¦¬íŠ¸ ì¸ë±ìŠ¤ë¥¼ env ì•¡ì…˜ìœ¼ë¡œ ë³€í™˜
        env_action = discrete_to_env_action(action_idx, obs)

        return {
            "turn": int(env_action.turn),
            "index": int(env_action.index),
            "power": float(env_action.power),
            "angle": float(env_action.angle),
        }

    def save(self, path: str):
        """
        í˜„ì¬ ë„¤íŠ¸ì›Œí¬ weightë¥¼ pathì— ì €ì¥.
        - í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ ë°”ë¡œ ì“¸ ìˆ˜ ìˆìŒ.
        """
        torch.save(self.net.state_dict(), path)

    @staticmethod
    def _load_net_from_path(path: str, device: torch.device | str):
        """
        ê³µí†µ: PolicyValueNet ìƒì„± + state_dict ë¡œë“œ
        """
        device = torch.device(device)
        net = PolicyValueNet()
        state_dict = torch.load(path, map_location=device)
        net.load_state_dict(state_dict)
        return net.to(device)


class YourBlackAgent(YourAlkkagiAgentBase):
    """
    í‘ ì—ì´ì „íŠ¸.
    - í‰ê°€ ì„œë²„ì—ì„œëŠ” ë³´í†µ YourBlackAgent.load(path)ë¡œ ë¶ˆëŸ¬ì„œ ì‚¬ìš©.
    """
    def __init__(self, net: PolicyValueNet, device: torch.device | str = "cpu"):
        super().__init__(net=net, color=0, device=device)

    @classmethod
    def load(cls, path: str) -> "kym.Agent":
        """
        pathì— ì €ì¥ëœ weightë¡œë¶€í„° í‘ ì—ì´ì „íŠ¸ í•˜ë‚˜ ìƒì„±.
        - ì˜ˆ: black_agent = YourBlackAgent.load("alkkagi_ppo.pt")
        """
        device = "cuda" if torch.cuda.is_available() else "cpu"
        net = YourAlkkagiAgentBase._load_net_from_path(path, device)
        return cls(net=net, device=device)


class YourWhiteAgent(YourAlkkagiAgentBase):
    """
    ë°± ì—ì´ì „íŠ¸.
    - í‰ê°€ ì„œë²„ì—ì„œëŠ” YourWhiteAgent.load(path)ë¡œ ë¶ˆëŸ¬ì„œ ì‚¬ìš©.
    """
    def __init__(self, net: PolicyValueNet, device: torch.device | str = "cpu"):
        super().__init__(net=net, color=1, device=device)

    @classmethod
    def load(cls, path: str) -> "kym.Agent":
        """
        pathì— ì €ì¥ëœ weightë¡œë¶€í„° ë°± ì—ì´ì „íŠ¸ í•˜ë‚˜ ìƒì„±.
        - ì˜ˆ: white_agent = YourWhiteAgent.load("alkkagi_ppo.pt")
        """
        device = "cuda" if torch.cuda.is_available() else "cpu"
        net = YourAlkkagiAgentBase._load_net_from_path(path, device)
        return cls(net=net, device=device)



# ================================================================
# 9. ê°„ë‹¨ í…ŒìŠ¤íŠ¸ìš© ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
# ================================================================

if __name__ == "__main__":
    device = "cuda" if torch.cuda.is_available() else "cpu"

    trained_net = train_ppo_selfplay(
        num_updates=1000,          # ëŒ€ëµ 5ë§Œ ì—í”¼ ì •ë„
        rollout_steps=2048,
        batch_size=512,
        device=device,
        save_path="alkkagi_ppo.pt",
        log_csv_path="training_metrics_v4.csv",
        checkpoint_every_episodes=1000,
        checkpoint_dir="checkpoints_v4",
    )

