{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db6404a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== [Epoch 1/10] =====\n",
      "[Ep 01-001] steps=8, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1516.0, opp_rating=1484.0\n",
      "[Ep 01-002] steps=8, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1498.5, opp_rating=1501.5\n",
      "[Ep 01-003] steps=12, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1482.7, opp_rating=1517.3\n",
      "[Ep 01-004] steps=5, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1500.3, opp_rating=1499.7\n",
      "[Ep 01-005] steps=9, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1484.2, opp_rating=1515.8\n",
      "[Ep 01-006] steps=10, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1501.7, opp_rating=1498.3\n",
      "[Ep 01-007] steps=7, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1485.5, opp_rating=1514.5\n",
      "[Ep 01-008] steps=13, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1470.9, opp_rating=1529.1\n",
      "[Ep 01-009] steps=5, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1457.5, opp_rating=1542.5\n",
      "[Ep 01-010] steps=11, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1445.4, opp_rating=1554.6\n",
      "[Ep 01-011] steps=9, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1466.2, opp_rating=1533.8\n",
      "[Ep 01-012] steps=5, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1485.3, opp_rating=1514.7\n",
      "[Ep 01-013] steps=5, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1470.6, opp_rating=1529.4\n",
      "[Ep 01-014] steps=10, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1489.3, opp_rating=1510.7\n",
      "[Ep 01-015] steps=8, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1474.3, opp_rating=1525.7\n",
      "[Ep 01-016] steps=9, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1460.7, opp_rating=1539.3\n",
      "[Ep 01-017] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1448.2, opp_rating=1551.8\n",
      "[Ep 01-018] steps=5, learner_color=1, R_learner=3.00, score=1.0, opp_id=snap_000, learner_rating=1468.9, opp_rating=1531.1\n",
      "[Ep 01-019] steps=13, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1455.7, opp_rating=1544.3\n",
      "[Ep 01-020] steps=9, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1475.7, opp_rating=1524.3\n",
      "[Ep 01-021] steps=5, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1461.9, opp_rating=1538.1\n",
      "[Ep 01-022] steps=5, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1449.4, opp_rating=1550.6\n",
      "[Ep 01-023] steps=14, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1469.9, opp_rating=1530.1\n",
      "[Ep 01-024] steps=13, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1488.6, opp_rating=1511.4\n",
      "[Ep 01-025] steps=6, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1505.7, opp_rating=1494.3\n",
      "[Ep 01-026] steps=13, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1521.2, opp_rating=1478.8\n",
      "[Ep 01-027] steps=5, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1535.2, opp_rating=1464.8\n",
      "[Ep 01-028] steps=7, learner_color=1, R_learner=3.00, score=1.0, opp_id=snap_000, learner_rating=1548.0, opp_rating=1452.0\n",
      "[Ep 01-029] steps=7, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1559.7, opp_rating=1440.3\n",
      "[Ep 01-030] steps=6, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1538.4, opp_rating=1461.6\n",
      "[Ep 01-031] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1518.9, opp_rating=1481.1\n",
      "[Ep 01-032] steps=17, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1533.2, opp_rating=1466.8\n",
      "[Ep 01-033] steps=10, learner_color=1, R_learner=-3.00, score=0.0, opp_id=snap_000, learner_rating=1514.2, opp_rating=1485.8\n",
      "[Ep 01-034] steps=12, learner_color=0, R_learner=3.00, score=1.0, opp_id=snap_000, learner_rating=1528.9, opp_rating=1471.1\n",
      "[Ep 01-035] steps=8, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1542.2, opp_rating=1457.8\n",
      "[Ep 01-036] steps=8, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1554.4, opp_rating=1445.6\n",
      "[Ep 01-037] steps=9, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1533.6, opp_rating=1466.4\n",
      "[Ep 01-038] steps=6, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1546.5, opp_rating=1453.5\n",
      "[Ep 01-039] steps=7, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1558.3, opp_rating=1441.7\n",
      "[Ep 01-040] steps=12, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1569.2, opp_rating=1430.8\n",
      "[Ep 01-041] steps=12, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1547.1, opp_rating=1452.9\n",
      "[Ep 01-042] steps=8, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1526.9, opp_rating=1473.1\n",
      "[Ep 01-043] steps=6, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1540.4, opp_rating=1459.6\n",
      "[Ep 01-044] steps=14, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1552.8, opp_rating=1447.2\n",
      "[Ep 01-045] steps=6, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1564.0, opp_rating=1436.0\n",
      "[Ep 01-046] steps=10, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1574.4, opp_rating=1425.6\n",
      "[Ep 01-047] steps=6, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1583.9, opp_rating=1416.1\n",
      "[Ep 01-048] steps=15, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1560.8, opp_rating=1439.2\n",
      "[Ep 01-049] steps=5, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1539.4, opp_rating=1460.6\n",
      "[Ep 01-050] steps=8, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1519.8, opp_rating=1480.2\n",
      "[Epoch 1] PPO UPDATE: total transitions = 219\n",
      "\n",
      "===== [Epoch 2/10] =====\n",
      "[Ep 02-001] steps=8, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1502.0, opp_rating=1498.0\n",
      "[Ep 02-002] steps=14, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1485.8, opp_rating=1514.2\n",
      "[Ep 02-003] steps=9, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1471.1, opp_rating=1528.9\n",
      "[Ep 02-004] steps=9, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1457.7, opp_rating=1542.3\n",
      "[Ep 02-005] steps=11, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1445.6, opp_rating=1554.4\n",
      "[Ep 02-006] steps=7, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1466.4, opp_rating=1533.6\n",
      "[Ep 02-007] steps=13, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1485.5, opp_rating=1514.5\n",
      "[Ep 02-008] steps=13, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1470.8, opp_rating=1529.2\n",
      "[Ep 02-009] steps=5, learner_color=1, R_learner=3.00, score=1.0, opp_id=snap_000, learner_rating=1489.5, opp_rating=1510.5\n",
      "[Ep 02-010] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1474.4, opp_rating=1525.6\n",
      "[Ep 02-011] steps=8, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1460.8, opp_rating=1539.2\n",
      "[Ep 02-012] steps=11, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1480.3, opp_rating=1519.7\n",
      "[Ep 02-013] steps=7, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1498.1, opp_rating=1501.9\n",
      "[Ep 02-014] steps=6, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1514.3, opp_rating=1485.7\n",
      "[Ep 02-015] steps=8, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1497.0, opp_rating=1503.0\n",
      "[Ep 02-016] steps=16, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1513.3, opp_rating=1486.7\n",
      "[Ep 02-017] steps=10, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1528.0, opp_rating=1472.0\n",
      "[Ep 02-018] steps=9, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1541.5, opp_rating=1458.5\n",
      "[Ep 02-019] steps=6, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1521.7, opp_rating=1478.3\n",
      "[Ep 02-020] steps=7, learner_color=0, R_learner=-3.00, score=0.0, opp_id=snap_000, learner_rating=1503.7, opp_rating=1496.3\n",
      "[Ep 02-021] steps=11, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1487.4, opp_rating=1512.6\n",
      "[Ep 02-022] steps=5, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1472.6, opp_rating=1527.4\n",
      "[Ep 02-023] steps=10, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1459.1, opp_rating=1540.9\n",
      "[Ep 02-024] steps=7, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1478.8, opp_rating=1521.2\n",
      "[Ep 02-025] steps=13, learner_color=1, R_learner=3.00, score=1.0, opp_id=snap_000, learner_rating=1496.7, opp_rating=1503.3\n",
      "[Ep 02-026] steps=5, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1481.0, opp_rating=1519.0\n",
      "[Ep 02-027] steps=8, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1466.8, opp_rating=1533.2\n",
      "[Ep 02-028] steps=18, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1485.8, opp_rating=1514.2\n",
      "[Ep 02-029] steps=10, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1503.1, opp_rating=1496.9\n",
      "[Ep 02-030] steps=7, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1486.8, opp_rating=1513.2\n",
      "[Ep 02-031] steps=5, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1472.0, opp_rating=1528.0\n",
      "[Ep 02-032] steps=5, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1490.6, opp_rating=1509.4\n",
      "[Ep 02-033] steps=6, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1475.4, opp_rating=1524.6\n",
      "[Ep 02-034] steps=26, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1493.7, opp_rating=1506.3\n",
      "[Ep 02-035] steps=14, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1510.3, opp_rating=1489.7\n",
      "[Ep 02-036] steps=5, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1493.3, opp_rating=1506.7\n",
      "[Ep 02-037] steps=10, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1509.9, opp_rating=1490.1\n",
      "[Ep 02-038] steps=8, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1493.0, opp_rating=1507.0\n",
      "[Ep 02-039] steps=7, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1509.7, opp_rating=1490.3\n",
      "[Ep 02-040] steps=10, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1492.8, opp_rating=1507.2\n",
      "[Ep 02-041] steps=9, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1509.4, opp_rating=1490.6\n",
      "[Ep 02-042] steps=12, learner_color=1, R_learner=-3.00, score=0.0, opp_id=snap_000, learner_rating=1492.6, opp_rating=1507.4\n",
      "[Ep 02-043] steps=9, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1509.3, opp_rating=1490.7\n",
      "[Ep 02-044] steps=5, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1524.4, opp_rating=1475.6\n",
      "[Ep 02-045] steps=5, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1538.2, opp_rating=1461.8\n",
      "[Ep 02-046] steps=12, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1518.7, opp_rating=1481.3\n",
      "[Ep 02-047] steps=11, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1533.0, opp_rating=1467.0\n",
      "[Ep 02-048] steps=6, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1546.0, opp_rating=1454.0\n",
      "[Ep 02-049] steps=10, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1525.9, opp_rating=1474.1\n",
      "[Ep 02-050] steps=11, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1507.5, opp_rating=1492.5\n",
      "[Epoch 2] PPO UPDATE: total transitions = 231\n",
      "[Epoch 2] New snapshot added: snap_001 (rating=1507.5)\n",
      "\n",
      "===== [Epoch 3/10] =====\n",
      "[Ep 03-001] steps=17, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_001, learner_rating=1523.5, opp_rating=1491.5\n",
      "[Ep 03-002] steps=12, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1506.1, opp_rating=1509.9\n",
      "[Ep 03-003] steps=14, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1522.2, opp_rating=1493.8\n",
      "[Ep 03-004] steps=7, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1536.9, opp_rating=1479.1\n",
      "[Ep 03-005] steps=17, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_001, learner_rating=1550.9, opp_rating=1477.6\n",
      "[Ep 03-006] steps=8, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1563.6, opp_rating=1466.3\n",
      "[Ep 03-007] steps=6, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1543.7, opp_rating=1497.5\n",
      "[Ep 03-008] steps=10, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_001, learner_rating=1525.6, opp_rating=1515.6\n",
      "[Ep 03-009] steps=5, learner_color=0, R_learner=-3.00, score=0.0, opp_id=snap_001, learner_rating=1509.1, opp_rating=1532.0\n",
      "[Ep 03-010] steps=5, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_001, learner_rating=1526.2, opp_rating=1515.0\n",
      "[Ep 03-011] steps=9, learner_color=1, R_learner=3.00, score=1.0, opp_id=snap_001, learner_rating=1541.7, opp_rating=1499.5\n",
      "[Ep 03-012] steps=6, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_001, learner_rating=1523.7, opp_rating=1517.4\n",
      "[Ep 03-013] steps=8, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_001, learner_rating=1539.4, opp_rating=1501.7\n",
      "[Ep 03-014] steps=7, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_001, learner_rating=1553.7, opp_rating=1487.4\n",
      "[Ep 03-015] steps=9, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_001, learner_rating=1534.7, opp_rating=1506.5\n",
      "[Ep 03-016] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_001, learner_rating=1517.4, opp_rating=1523.8\n",
      "[Ep 03-017] steps=9, learner_color=1, R_learner=3.00, score=1.0, opp_id=snap_001, learner_rating=1533.7, opp_rating=1507.5\n",
      "[Ep 03-018] steps=10, learner_color=0, R_learner=3.00, score=1.0, opp_id=snap_001, learner_rating=1548.5, opp_rating=1492.7\n",
      "[Ep 03-019] steps=13, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1529.9, opp_rating=1511.2\n",
      "[Ep 03-020] steps=8, learner_color=0, R_learner=3.00, score=1.0, opp_id=snap_001, learner_rating=1545.1, opp_rating=1496.1\n",
      "[Ep 03-021] steps=7, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1526.8, opp_rating=1514.3\n",
      "[Ep 03-022] steps=8, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1510.3, opp_rating=1530.9\n",
      "[Ep 03-023] steps=11, learner_color=0, R_learner=-3.00, score=0.0, opp_id=snap_001, learner_rating=1495.2, opp_rating=1546.0\n",
      "[Ep 03-024] steps=5, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1477.9, opp_rating=1483.7\n",
      "[Ep 03-025] steps=10, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1494.2, opp_rating=1467.4\n",
      "[Ep 03-026] steps=12, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1476.9, opp_rating=1484.6\n",
      "[Ep 03-027] steps=8, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1493.3, opp_rating=1468.3\n",
      "[Ep 03-028] steps=9, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1508.1, opp_rating=1453.4\n",
      "[Ep 03-029] steps=9, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_001, learner_rating=1525.9, opp_rating=1528.2\n",
      "[Ep 03-030] steps=8, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_001, learner_rating=1542.0, opp_rating=1512.1\n",
      "[Ep 03-031] steps=14, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1524.6, opp_rating=1529.5\n",
      "[Ep 03-032] steps=19, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1508.8, opp_rating=1545.3\n",
      "[Ep 03-033] steps=13, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1494.5, opp_rating=1559.6\n",
      "[Ep 03-034] steps=6, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1476.6, opp_rating=1471.3\n",
      "[Ep 03-035] steps=13, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1492.4, opp_rating=1455.5\n",
      "[Ep 03-036] steps=7, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1506.7, opp_rating=1441.2\n",
      "[Ep 03-037] steps=14, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1493.1, opp_rating=1573.2\n",
      "[Ep 03-038] steps=8, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1474.7, opp_rating=1459.6\n",
      "[Ep 03-039] steps=11, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1458.0, opp_rating=1476.3\n",
      "[Ep 03-040] steps=14, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1442.9, opp_rating=1491.5\n",
      "[Ep 03-041] steps=13, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1429.1, opp_rating=1505.2\n",
      "[Ep 03-042] steps=6, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1448.5, opp_rating=1485.8\n",
      "[Ep 03-043] steps=5, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1466.3, opp_rating=1468.1\n",
      "[Ep 03-044] steps=12, learner_color=1, R_learner=-3.00, score=0.0, opp_id=snap_000, learner_rating=1450.3, opp_rating=1484.0\n",
      "[Ep 03-045] steps=8, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1467.9, opp_rating=1466.4\n",
      "[Ep 03-046] steps=9, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1483.8, opp_rating=1450.5\n",
      "[Ep 03-047] steps=9, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1466.3, opp_rating=1468.0\n",
      "[Ep 03-048] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1450.4, opp_rating=1484.0\n",
      "[Ep 03-049] steps=6, learner_color=1, R_learner=-3.00, score=0.0, opp_id=snap_000, learner_rating=1435.9, opp_rating=1498.4\n",
      "[Ep 03-050] steps=11, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1454.8, opp_rating=1479.6\n",
      "[Epoch 3] PPO UPDATE: total transitions = 239\n",
      "\n",
      "===== [Epoch 4/10] =====\n",
      "[Ep 04-001] steps=5, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1471.9, opp_rating=1462.4\n",
      "[Ep 04-002] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1455.5, opp_rating=1478.9\n",
      "[Ep 04-003] steps=7, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1440.5, opp_rating=1493.8\n",
      "[Ep 04-004] steps=5, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1427.0, opp_rating=1507.4\n",
      "[Ep 04-005] steps=15, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1446.6, opp_rating=1487.7\n",
      "[Ep 04-006] steps=9, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1432.5, opp_rating=1501.8\n",
      "[Ep 04-007] steps=19, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1451.6, opp_rating=1482.7\n",
      "[Ep 04-008] steps=11, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1437.1, opp_rating=1497.3\n",
      "[Ep 04-009] steps=6, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1423.8, opp_rating=1510.5\n",
      "[Ep 04-010] steps=12, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1443.7, opp_rating=1490.6\n",
      "[Ep 04-011] steps=5, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1429.9, opp_rating=1504.5\n",
      "[Ep 04-012] steps=5, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1449.3, opp_rating=1485.1\n",
      "[Ep 04-013] steps=8, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1466.9, opp_rating=1467.4\n",
      "[Ep 04-014] steps=5, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1450.9, opp_rating=1483.4\n",
      "[Ep 04-015] steps=8, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1468.4, opp_rating=1465.9\n",
      "[Ep 04-016] steps=6, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1484.3, opp_rating=1450.0\n",
      "[Ep 04-017] steps=9, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1498.7, opp_rating=1435.6\n",
      "[Ep 04-018] steps=8, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1511.8, opp_rating=1422.5\n",
      "[Ep 04-019] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_001, learner_rating=1498.6, opp_rating=1586.4\n",
      "[Ep 04-020] steps=7, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1511.2, opp_rating=1409.9\n",
      "[Ep 04-021] steps=5, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1498.6, opp_rating=1599.0\n",
      "[Ep 04-022] steps=9, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1510.6, opp_rating=1397.9\n",
      "[Ep 04-023] steps=12, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_001, learner_rating=1530.6, opp_rating=1579.0\n",
      "[Ep 04-024] steps=14, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_001, learner_rating=1548.8, opp_rating=1560.8\n",
      "[Ep 04-025] steps=7, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1533.4, opp_rating=1576.2\n",
      "[Ep 04-026] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_001, learner_rating=1519.3, opp_rating=1590.2\n",
      "[Ep 04-027] steps=10, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1506.5, opp_rating=1603.0\n",
      "[Ep 04-028] steps=8, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_001, learner_rating=1526.9, opp_rating=1582.7\n",
      "[Ep 04-029] steps=7, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1513.4, opp_rating=1596.1\n",
      "[Ep 04-030] steps=11, learner_color=1, R_learner=3.00, score=1.0, opp_id=snap_001, learner_rating=1533.2, opp_rating=1576.4\n",
      "[Ep 04-031] steps=8, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_001, learner_rating=1519.1, opp_rating=1590.4\n",
      "[Ep 04-032] steps=9, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1506.4, opp_rating=1603.2\n",
      "[Ep 04-033] steps=12, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1494.7, opp_rating=1614.8\n",
      "[Ep 04-034] steps=5, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1474.4, opp_rating=1418.3\n",
      "[Ep 04-035] steps=9, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1455.8, opp_rating=1436.8\n",
      "[Ep 04-036] steps=10, learner_color=1, R_learner=-3.00, score=0.0, opp_id=snap_000, learner_rating=1438.9, opp_rating=1453.7\n",
      "[Ep 04-037] steps=11, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1423.6, opp_rating=1469.0\n",
      "[Ep 04-038] steps=7, learner_color=1, R_learner=3.00, score=1.0, opp_id=snap_000, learner_rating=1441.7, opp_rating=1450.9\n",
      "[Ep 04-039] steps=5, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1426.1, opp_rating=1466.5\n",
      "[Ep 04-040] steps=12, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1412.0, opp_rating=1480.7\n",
      "[Ep 04-041] steps=13, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1399.1, opp_rating=1493.5\n",
      "[Ep 04-042] steps=6, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1387.3, opp_rating=1505.3\n",
      "[Ep 04-043] steps=12, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1408.6, opp_rating=1484.1\n",
      "[Ep 04-044] steps=11, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1396.0, opp_rating=1496.6\n",
      "[Ep 04-045] steps=6, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1384.5, opp_rating=1508.1\n",
      "[Ep 04-046] steps=10, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1374.0, opp_rating=1518.7\n",
      "[Ep 04-047] steps=11, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1364.3, opp_rating=1528.4\n",
      "[Ep 04-048] steps=6, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1387.3, opp_rating=1505.3\n",
      "[Ep 04-049] steps=8, learner_color=1, R_learner=-3.00, score=0.0, opp_id=snap_000, learner_rating=1376.6, opp_rating=1516.1\n",
      "[Ep 04-050] steps=9, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1366.7, opp_rating=1526.0\n",
      "[Epoch 4] PPO UPDATE: total transitions = 223\n",
      "[Epoch 4] New snapshot added: snap_002 (rating=1366.7)\n",
      "\n",
      "===== [Epoch 5/10] =====\n",
      "[Ep 05-001] steps=7, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1382.7, opp_rating=1350.7\n",
      "[Ep 05-002] steps=11, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1365.2, opp_rating=1368.1\n",
      "[Ep 05-003] steps=5, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1381.3, opp_rating=1352.0\n",
      "[Ep 05-004] steps=5, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1364.0, opp_rating=1369.3\n",
      "[Ep 05-005] steps=11, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1348.2, opp_rating=1385.1\n",
      "[Ep 05-006] steps=8, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1365.9, opp_rating=1367.4\n",
      "[Ep 05-007] steps=7, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1350.0, opp_rating=1383.3\n",
      "[Ep 05-008] steps=5, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1367.5, opp_rating=1365.8\n",
      "[Ep 05-009] steps=13, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1383.4, opp_rating=1349.9\n",
      "[Ep 05-010] steps=7, learner_color=1, R_learner=3.00, score=1.0, opp_id=snap_002, learner_rating=1397.9, opp_rating=1335.4\n",
      "[Ep 05-011] steps=8, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1379.0, opp_rating=1354.3\n",
      "[Ep 05-012] steps=15, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1361.9, opp_rating=1371.4\n",
      "[Ep 05-013] steps=8, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1378.3, opp_rating=1355.0\n",
      "[Ep 05-014] steps=5, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1361.3, opp_rating=1372.0\n",
      "[Ep 05-015] steps=10, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1377.8, opp_rating=1355.5\n",
      "[Ep 05-016] steps=9, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1392.7, opp_rating=1340.6\n",
      "[Ep 05-017] steps=5, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1374.4, opp_rating=1359.0\n",
      "[Ep 05-018] steps=9, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1389.6, opp_rating=1343.7\n",
      "[Ep 05-019] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1371.5, opp_rating=1361.8\n",
      "[Ep 05-020] steps=13, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1355.1, opp_rating=1378.2\n",
      "[Ep 05-021] steps=8, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1340.2, opp_rating=1393.2\n",
      "[Ep 05-022] steps=8, learner_color=1, R_learner=-3.00, score=0.0, opp_id=snap_002, learner_rating=1326.6, opp_rating=1406.7\n",
      "[Ep 05-023] steps=7, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1346.2, opp_rating=1387.1\n",
      "[Ep 05-024] steps=12, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1332.1, opp_rating=1401.2\n",
      "[Ep 05-025] steps=9, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1351.2, opp_rating=1382.1\n",
      "[Ep 05-026] steps=9, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1368.6, opp_rating=1364.7\n",
      "[Ep 05-027] steps=6, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1384.5, opp_rating=1348.9\n",
      "[Ep 05-028] steps=14, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1398.8, opp_rating=1334.5\n",
      "[Ep 05-029] steps=7, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1411.9, opp_rating=1321.4\n",
      "[Ep 05-030] steps=14, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1391.8, opp_rating=1341.5\n",
      "[Ep 05-031] steps=9, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1405.5, opp_rating=1327.8\n",
      "[Ep 05-032] steps=8, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1418.0, opp_rating=1315.3\n",
      "[Ep 05-033] steps=6, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1429.4, opp_rating=1303.9\n",
      "[Ep 05-034] steps=11, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1417.7, opp_rating=1537.7\n",
      "[Ep 05-035] steps=8, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1428.7, opp_rating=1293.0\n",
      "[Ep 05-036] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1417.5, opp_rating=1548.8\n",
      "[Ep 05-037] steps=6, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1396.0, opp_rating=1314.5\n",
      "[Ep 05-038] steps=8, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1376.3, opp_rating=1334.2\n",
      "[Ep 05-039] steps=8, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1358.4, opp_rating=1352.1\n",
      "[Ep 05-040] steps=5, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1342.1, opp_rating=1368.4\n",
      "[Ep 05-041] steps=11, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1359.3, opp_rating=1351.2\n",
      "[Ep 05-042] steps=7, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1375.0, opp_rating=1335.6\n",
      "[Ep 05-043] steps=16, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1389.1, opp_rating=1321.4\n",
      "[Ep 05-044] steps=13, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1370.1, opp_rating=1340.4\n",
      "[Ep 05-045] steps=13, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1352.7, opp_rating=1357.8\n",
      "[Ep 05-046] steps=11, learner_color=0, R_learner=-3.00, score=0.0, opp_id=snap_002, learner_rating=1336.9, opp_rating=1373.6\n",
      "[Ep 05-047] steps=6, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1354.6, opp_rating=1355.9\n",
      "[Ep 05-048] steps=18, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1370.7, opp_rating=1339.8\n",
      "[Ep 05-049] steps=8, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1385.3, opp_rating=1325.2\n",
      "[Ep 05-050] steps=10, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1366.5, opp_rating=1344.0\n",
      "[Epoch 5] PPO UPDATE: total transitions = 226\n",
      "\n",
      "===== [Epoch 6/10] =====\n",
      "[Ep 06-001] steps=10, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1381.5, opp_rating=1329.0\n",
      "[Ep 06-002] steps=8, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1395.1, opp_rating=1315.4\n",
      "[Ep 06-003] steps=13, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1375.5, opp_rating=1335.0\n",
      "[Ep 06-004] steps=11, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1389.6, opp_rating=1320.9\n",
      "[Ep 06-005] steps=13, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1370.5, opp_rating=1340.0\n",
      "[Ep 06-006] steps=11, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1353.1, opp_rating=1357.4\n",
      "[Ep 06-007] steps=13, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1369.3, opp_rating=1341.2\n",
      "[Ep 06-008] steps=9, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1384.0, opp_rating=1326.5\n",
      "[Ep 06-009] steps=9, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1397.4, opp_rating=1313.1\n",
      "[Ep 06-010] steps=12, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1409.6, opp_rating=1300.9\n",
      "[Ep 06-011] steps=9, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1388.7, opp_rating=1321.8\n",
      "[Ep 06-012] steps=18, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1369.7, opp_rating=1340.8\n",
      "[Ep 06-013] steps=10, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1384.4, opp_rating=1326.1\n",
      "[Ep 06-014] steps=9, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1397.7, opp_rating=1312.8\n",
      "[Ep 06-015] steps=17, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1409.9, opp_rating=1300.6\n",
      "[Ep 06-016] steps=11, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1389.0, opp_rating=1321.5\n",
      "[Ep 06-017] steps=7, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1401.9, opp_rating=1308.6\n",
      "[Ep 06-018] steps=9, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1381.7, opp_rating=1328.8\n",
      "[Ep 06-019] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1363.3, opp_rating=1347.2\n",
      "[Ep 06-020] steps=11, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1378.6, opp_rating=1331.9\n",
      "[Ep 06-021] steps=10, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1392.4, opp_rating=1318.1\n",
      "[Ep 06-022] steps=13, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1405.1, opp_rating=1305.4\n",
      "[Ep 06-023] steps=12, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1384.6, opp_rating=1325.9\n",
      "[Ep 06-024] steps=9, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1365.9, opp_rating=1344.6\n",
      "[Ep 06-025] steps=9, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1380.9, opp_rating=1329.6\n",
      "[Ep 06-026] steps=7, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1394.6, opp_rating=1315.9\n",
      "[Ep 06-027] steps=10, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1407.0, opp_rating=1303.5\n",
      "[Ep 06-028] steps=6, learner_color=0, R_learner=3.00, score=1.0, opp_id=snap_002, learner_rating=1418.4, opp_rating=1292.1\n",
      "[Ep 06-029] steps=8, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1396.8, opp_rating=1313.7\n",
      "[Ep 06-030] steps=11, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1409.1, opp_rating=1301.4\n",
      "[Ep 06-031] steps=6, learner_color=0, R_learner=3.00, score=1.0, opp_id=snap_002, learner_rating=1420.3, opp_rating=1290.2\n",
      "[Ep 06-032] steps=11, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1441.9, opp_rating=1527.1\n",
      "[Ep 06-033] steps=7, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1429.8, opp_rating=1539.3\n",
      "[Ep 06-034] steps=10, learner_color=0, R_learner=3.00, score=1.0, opp_id=snap_000, learner_rating=1450.7, opp_rating=1518.4\n",
      "[Ep 06-035] steps=9, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1469.7, opp_rating=1499.3\n",
      "[Ep 06-036] steps=10, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1487.1, opp_rating=1482.0\n",
      "[Ep 06-037] steps=9, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1502.9, opp_rating=1466.2\n",
      "[Ep 06-038] steps=9, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1485.2, opp_rating=1483.9\n",
      "[Ep 06-039] steps=7, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1469.1, opp_rating=1499.9\n",
      "[Ep 06-040] steps=10, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1486.5, opp_rating=1482.5\n",
      "[Ep 06-041] steps=14, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1470.3, opp_rating=1498.7\n",
      "[Ep 06-042] steps=6, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1455.7, opp_rating=1513.4\n",
      "[Ep 06-043] steps=8, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1474.3, opp_rating=1494.8\n",
      "[Ep 06-044] steps=8, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1491.2, opp_rating=1477.8\n",
      "[Ep 06-045] steps=8, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1506.6, opp_rating=1462.4\n",
      "[Ep 06-046] steps=8, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1488.6, opp_rating=1480.5\n",
      "[Ep 06-047] steps=8, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1472.2, opp_rating=1496.8\n",
      "[Ep 06-048] steps=8, learner_color=0, R_learner=3.00, score=1.0, opp_id=snap_000, learner_rating=1489.3, opp_rating=1479.7\n",
      "[Ep 06-049] steps=14, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1472.9, opp_rating=1496.2\n",
      "[Ep 06-050] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1458.0, opp_rating=1511.1\n",
      "[Epoch 6] PPO UPDATE: total transitions = 243\n",
      "[Epoch 6] New snapshot added: snap_003 (rating=1458.0)\n",
      "\n",
      "===== [Epoch 7/10] =====\n",
      "[Ep 07-001] steps=16, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_003, learner_rating=1442.0, opp_rating=1474.0\n",
      "[Ep 07-002] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_003, learner_rating=1427.4, opp_rating=1488.5\n",
      "[Ep 07-003] steps=12, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_003, learner_rating=1414.2, opp_rating=1501.7\n",
      "[Ep 07-004] steps=10, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_003, learner_rating=1402.2, opp_rating=1513.8\n",
      "[Ep 07-005] steps=6, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1423.0, opp_rating=1490.2\n",
      "[Ep 07-006] steps=15, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1410.1, opp_rating=1503.2\n",
      "[Ep 07-007] steps=9, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1398.3, opp_rating=1515.0\n",
      "[Ep 07-008] steps=8, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1377.5, opp_rating=1311.1\n",
      "[Ep 07-009] steps=8, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1390.4, opp_rating=1298.1\n",
      "[Ep 07-010] steps=9, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1402.3, opp_rating=1286.2\n",
      "[Ep 07-011] steps=7, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_003, learner_rating=1423.2, opp_rating=1492.8\n",
      "[Ep 07-012] steps=14, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_003, learner_rating=1410.4, opp_rating=1505.6\n",
      "[Ep 07-013] steps=8, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_003, learner_rating=1430.7, opp_rating=1485.4\n",
      "[Ep 07-014] steps=11, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_003, learner_rating=1449.2, opp_rating=1466.9\n",
      "[Ep 07-015] steps=8, learner_color=1, R_learner=-3.00, score=0.0, opp_id=snap_003, learner_rating=1434.0, opp_rating=1482.1\n",
      "[Ep 07-016] steps=9, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_003, learner_rating=1420.2, opp_rating=1495.9\n",
      "[Ep 07-017] steps=7, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_003, learner_rating=1439.6, opp_rating=1476.4\n",
      "[Ep 07-018] steps=11, learner_color=0, R_learner=-3.00, score=0.0, opp_id=snap_003, learner_rating=1425.3, opp_rating=1490.7\n",
      "[Ep 07-019] steps=9, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_003, learner_rating=1444.3, opp_rating=1471.8\n",
      "[Ep 07-020] steps=5, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_003, learner_rating=1429.6, opp_rating=1486.5\n",
      "[Ep 07-021] steps=6, learner_color=1, R_learner=-3.00, score=0.0, opp_id=snap_003, learner_rating=1416.2, opp_rating=1499.9\n",
      "[Ep 07-022] steps=8, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_003, learner_rating=1403.9, opp_rating=1512.1\n",
      "[Ep 07-023] steps=10, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_003, learner_rating=1392.8, opp_rating=1523.3\n",
      "[Ep 07-024] steps=8, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1372.0, opp_rating=1307.0\n",
      "[Ep 07-025] steps=7, learner_color=1, R_learner=3.00, score=1.0, opp_id=snap_002, learner_rating=1385.0, opp_rating=1294.0\n",
      "[Ep 07-026] steps=9, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1364.9, opp_rating=1314.1\n",
      "[Ep 07-027] steps=17, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1346.6, opp_rating=1332.4\n",
      "[Ep 07-028] steps=14, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1330.0, opp_rating=1349.0\n",
      "[Ep 07-029] steps=7, learner_color=1, R_learner=3.00, score=1.0, opp_id=snap_002, learner_rating=1346.8, opp_rating=1332.2\n",
      "[Ep 07-030] steps=6, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1330.2, opp_rating=1348.8\n",
      "[Ep 07-031] steps=14, learner_color=0, R_learner=3.00, score=1.0, opp_id=snap_002, learner_rating=1347.0, opp_rating=1332.0\n",
      "[Ep 07-032] steps=15, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1330.3, opp_rating=1348.7\n",
      "[Ep 07-033] steps=5, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1347.2, opp_rating=1331.8\n",
      "[Ep 07-034] steps=5, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1330.5, opp_rating=1348.5\n",
      "[Ep 07-035] steps=8, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1315.3, opp_rating=1363.7\n",
      "[Ep 07-036] steps=9, learner_color=0, R_learner=-3.00, score=0.0, opp_id=snap_002, learner_rating=1301.5, opp_rating=1377.5\n",
      "[Ep 07-037] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1289.0, opp_rating=1390.0\n",
      "[Ep 07-038] steps=9, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1277.5, opp_rating=1401.5\n",
      "[Ep 07-039] steps=8, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1299.0, opp_rating=1380.0\n",
      "[Ep 07-040] steps=6, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1318.6, opp_rating=1360.4\n",
      "[Ep 07-041] steps=12, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1304.5, opp_rating=1374.5\n",
      "[Ep 07-042] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1291.7, opp_rating=1387.3\n",
      "[Ep 07-043] steps=6, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1280.0, opp_rating=1399.0\n",
      "[Ep 07-044] steps=8, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1301.3, opp_rating=1377.7\n",
      "[Ep 07-045] steps=7, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1320.8, opp_rating=1358.2\n",
      "[Ep 07-046] steps=9, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1306.5, opp_rating=1372.5\n",
      "[Ep 07-047] steps=6, learner_color=1, R_learner=-3.00, score=0.0, opp_id=snap_002, learner_rating=1293.5, opp_rating=1385.5\n",
      "[Ep 07-048] steps=7, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1281.6, opp_rating=1397.4\n",
      "[Ep 07-049] steps=9, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1302.8, opp_rating=1376.2\n",
      "[Ep 07-050] steps=10, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1290.1, opp_rating=1388.9\n",
      "[Epoch 7] PPO UPDATE: total transitions = 227\n",
      "\n",
      "===== [Epoch 8/10] =====\n",
      "[Ep 08-001] steps=12, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1310.5, opp_rating=1368.5\n",
      "[Ep 08-002] steps=5, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1297.2, opp_rating=1381.8\n",
      "[Ep 08-003] steps=17, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1285.0, opp_rating=1394.0\n",
      "[Ep 08-004] steps=10, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1305.9, opp_rating=1373.1\n",
      "[Ep 08-005] steps=13, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1324.9, opp_rating=1354.1\n",
      "[Ep 08-006] steps=8, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1342.3, opp_rating=1336.7\n",
      "[Ep 08-007] steps=5, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1358.0, opp_rating=1321.0\n",
      "[Ep 08-008] steps=11, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1340.3, opp_rating=1338.7\n",
      "[Ep 08-009] steps=5, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1324.2, opp_rating=1354.8\n",
      "[Ep 08-010] steps=10, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1309.6, opp_rating=1369.4\n",
      "[Ep 08-011] steps=8, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1296.4, opp_rating=1382.6\n",
      "[Ep 08-012] steps=6, learner_color=1, R_learner=-3.00, score=0.0, opp_id=snap_002, learner_rating=1284.3, opp_rating=1394.8\n",
      "[Ep 08-013] steps=9, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1305.2, opp_rating=1373.8\n",
      "[Ep 08-014] steps=8, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1292.3, opp_rating=1386.7\n",
      "[Ep 08-015] steps=7, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1312.5, opp_rating=1366.5\n",
      "[Ep 08-016] steps=12, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1331.0, opp_rating=1348.0\n",
      "[Ep 08-017] steps=8, learner_color=1, R_learner=-3.00, score=0.0, opp_id=snap_002, learner_rating=1315.8, opp_rating=1363.2\n",
      "[Ep 08-018] steps=5, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1302.0, opp_rating=1377.0\n",
      "[Ep 08-019] steps=6, learner_color=1, R_learner=-3.00, score=0.0, opp_id=snap_002, learner_rating=1289.4, opp_rating=1389.6\n",
      "[Ep 08-020] steps=9, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1277.9, opp_rating=1401.1\n",
      "[Ep 08-021] steps=10, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1267.3, opp_rating=1411.7\n",
      "[Ep 08-022] steps=20, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1257.6, opp_rating=1421.4\n",
      "[Ep 08-023] steps=13, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1280.6, opp_rating=1398.4\n",
      "[Ep 08-024] steps=5, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1301.9, opp_rating=1377.2\n",
      "[Ep 08-025] steps=5, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1289.3, opp_rating=1389.7\n",
      "[Ep 08-026] steps=7, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1309.8, opp_rating=1369.2\n",
      "[Ep 08-027] steps=5, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1296.5, opp_rating=1382.5\n",
      "[Ep 08-028] steps=16, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1316.4, opp_rating=1362.6\n",
      "[Ep 08-029] steps=10, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1334.5, opp_rating=1344.5\n",
      "[Ep 08-030] steps=6, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1350.9, opp_rating=1328.1\n",
      "[Ep 08-031] steps=10, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1333.9, opp_rating=1345.1\n",
      "[Ep 08-032] steps=19, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1350.4, opp_rating=1328.6\n",
      "[Ep 08-033] steps=8, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1333.4, opp_rating=1345.6\n",
      "[Ep 08-034] steps=5, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1350.0, opp_rating=1329.0\n",
      "[Ep 08-035] steps=18, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1333.0, opp_rating=1346.0\n",
      "[Ep 08-036] steps=10, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1317.6, opp_rating=1361.4\n",
      "[Ep 08-037] steps=9, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1303.6, opp_rating=1375.4\n",
      "[Ep 08-038] steps=7, learner_color=0, R_learner=-3.00, score=0.0, opp_id=snap_002, learner_rating=1290.9, opp_rating=1388.1\n",
      "[Ep 08-039] steps=5, learner_color=0, R_learner=-3.00, score=0.0, opp_id=snap_002, learner_rating=1279.2, opp_rating=1399.8\n",
      "[Ep 08-040] steps=17, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1268.6, opp_rating=1410.4\n",
      "[Ep 08-041] steps=6, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1290.8, opp_rating=1388.2\n",
      "[Ep 08-042] steps=12, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1279.1, opp_rating=1399.9\n",
      "[Ep 08-043] steps=10, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1268.5, opp_rating=1410.5\n",
      "[Ep 08-044] steps=9, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1290.7, opp_rating=1388.3\n",
      "[Ep 08-045] steps=6, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1311.1, opp_rating=1367.9\n",
      "[Ep 08-046] steps=6, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1329.7, opp_rating=1349.3\n",
      "[Ep 08-047] steps=9, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1314.6, opp_rating=1364.4\n",
      "[Ep 08-048] steps=9, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_002, learner_rating=1300.8, opp_rating=1378.2\n",
      "[Ep 08-049] steps=14, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1320.4, opp_rating=1358.7\n",
      "[Ep 08-050] steps=10, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1338.1, opp_rating=1340.9\n",
      "[Epoch 8] PPO UPDATE: total transitions = 237\n",
      "[Epoch 8] New snapshot added: snap_004 (rating=1338.1)\n",
      "\n",
      "===== [Epoch 9/10] =====\n",
      "[Ep 09-001] steps=13, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_004, learner_rating=1354.1, opp_rating=1322.1\n",
      "[Ep 09-002] steps=7, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1369.5, opp_rating=1325.5\n",
      "[Ep 09-003] steps=7, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1351.5, opp_rating=1343.5\n",
      "[Ep 09-004] steps=15, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1335.1, opp_rating=1359.9\n",
      "[Ep 09-005] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_004, learner_rating=1318.5, opp_rating=1338.7\n",
      "[Ep 09-006] steps=8, learner_color=1, R_learner=-3.00, score=0.0, opp_id=snap_004, learner_rating=1303.4, opp_rating=1353.8\n",
      "[Ep 09-007] steps=10, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_004, learner_rating=1321.7, opp_rating=1335.5\n",
      "[Ep 09-008] steps=7, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_004, learner_rating=1338.4, opp_rating=1318.8\n",
      "[Ep 09-009] steps=11, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_004, learner_rating=1353.5, opp_rating=1303.7\n",
      "[Ep 09-010] steps=7, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1369.8, opp_rating=1343.6\n",
      "[Ep 09-011] steps=9, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1384.6, opp_rating=1328.8\n",
      "[Ep 09-012] steps=10, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_002, learner_rating=1366.0, opp_rating=1347.3\n",
      "[Ep 09-013] steps=11, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1381.2, opp_rating=1332.2\n",
      "[Ep 09-014] steps=14, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_002, learner_rating=1394.9, opp_rating=1318.4\n",
      "[Ep 09-015] steps=11, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_002, learner_rating=1407.5, opp_rating=1305.9\n",
      "[Ep 09-016] steps=13, learner_color=1, R_learner=3.00, score=1.0, opp_id=snap_002, learner_rating=1418.9, opp_rating=1294.5\n",
      "[Ep 09-017] steps=15, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1439.2, opp_rating=1494.7\n",
      "[Ep 09-018] steps=14, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1457.8, opp_rating=1476.1\n",
      "[Ep 09-019] steps=10, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1442.6, opp_rating=1491.3\n",
      "[Ep 09-020] steps=7, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1460.8, opp_rating=1473.1\n",
      "[Ep 09-021] steps=6, learner_color=0, R_learner=3.00, score=1.0, opp_id=snap_000, learner_rating=1477.4, opp_rating=1456.5\n",
      "[Ep 09-022] steps=14, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1460.4, opp_rating=1473.5\n",
      "[Ep 09-023] steps=9, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1477.0, opp_rating=1456.9\n",
      "[Ep 09-024] steps=5, learner_color=1, R_learner=3.00, score=1.0, opp_id=snap_000, learner_rating=1492.1, opp_rating=1441.8\n",
      "[Ep 09-025] steps=15, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_003, learner_rating=1509.5, opp_rating=1505.9\n",
      "[Ep 09-026] steps=5, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_003, learner_rating=1493.4, opp_rating=1522.0\n",
      "[Ep 09-027] steps=6, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_003, learner_rating=1510.7, opp_rating=1504.7\n",
      "[Ep 09-028] steps=10, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_003, learner_rating=1494.4, opp_rating=1521.0\n",
      "[Ep 09-029] steps=8, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_003, learner_rating=1479.6, opp_rating=1535.8\n",
      "[Ep 09-030] steps=5, learner_color=0, R_learner=-3.00, score=0.0, opp_id=snap_000, learner_rating=1461.9, opp_rating=1459.5\n",
      "[Ep 09-031] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_000, learner_rating=1445.8, opp_rating=1475.6\n",
      "[Ep 09-032] steps=8, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1463.2, opp_rating=1458.3\n",
      "[Ep 09-033] steps=8, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1478.9, opp_rating=1442.5\n",
      "[Ep 09-034] steps=5, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1493.3, opp_rating=1428.2\n",
      "[Ep 09-035] steps=12, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_003, learner_rating=1511.2, opp_rating=1517.8\n",
      "[Ep 09-036] steps=5, learner_color=0, R_learner=-3.00, score=0.0, opp_id=snap_003, learner_rating=1495.5, opp_rating=1533.5\n",
      "[Ep 09-037] steps=7, learner_color=1, R_learner=3.00, score=1.0, opp_id=snap_003, learner_rating=1513.3, opp_rating=1515.8\n",
      "[Ep 09-038] steps=7, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_003, learner_rating=1529.4, opp_rating=1499.7\n",
      "[Ep 09-039] steps=6, learner_color=0, R_learner=3.00, score=1.0, opp_id=snap_003, learner_rating=1544.0, opp_rating=1485.0\n",
      "[Ep 09-040] steps=11, learner_color=1, R_learner=3.00, score=1.0, opp_id=snap_003, learner_rating=1557.3, opp_rating=1471.7\n",
      "[Ep 09-041] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_001, learner_rating=1543.9, opp_rating=1628.2\n",
      "[Ep 09-042] steps=8, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_003, learner_rating=1556.7, opp_rating=1459.0\n",
      "[Ep 09-043] steps=8, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_001, learner_rating=1575.9, opp_rating=1609.0\n",
      "[Ep 09-044] steps=9, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_001, learner_rating=1593.4, opp_rating=1591.4\n",
      "[Ep 09-045] steps=14, learner_color=1, R_learner=-3.00, score=0.0, opp_id=snap_001, learner_rating=1577.3, opp_rating=1607.5\n",
      "[Ep 09-046] steps=6, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_001, learner_rating=1594.7, opp_rating=1590.2\n",
      "[Ep 09-047] steps=9, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_001, learner_rating=1578.5, opp_rating=1606.4\n",
      "[Ep 09-048] steps=9, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_001, learner_rating=1595.8, opp_rating=1589.1\n",
      "[Ep 09-049] steps=5, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_001, learner_rating=1611.5, opp_rating=1573.4\n",
      "[Ep 09-050] steps=7, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_001, learner_rating=1625.7, opp_rating=1559.1\n",
      "[Epoch 9] PPO UPDATE: total transitions = 217\n",
      "\n",
      "===== [Epoch 10/10] =====\n",
      "[Ep 10-001] steps=7, learner_color=1, R_learner=3.00, score=1.0, opp_id=snap_001, learner_rating=1638.7, opp_rating=1546.2\n",
      "[Ep 10-002] steps=8, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_001, learner_rating=1650.5, opp_rating=1534.3\n",
      "[Ep 10-003] steps=5, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_001, learner_rating=1661.4, opp_rating=1523.5\n",
      "[Ep 10-004] steps=9, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_001, learner_rating=1639.3, opp_rating=1545.5\n",
      "[Ep 10-005] steps=7, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_001, learner_rating=1651.1, opp_rating=1533.7\n",
      "[Ep 10-006] steps=5, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1629.9, opp_rating=1555.0\n",
      "[Ep 10-007] steps=14, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1610.5, opp_rating=1574.4\n",
      "[Ep 10-008] steps=7, learner_color=0, R_learner=-3.00, score=0.0, opp_id=snap_001, learner_rating=1592.9, opp_rating=1592.0\n",
      "[Ep 10-009] steps=9, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_001, learner_rating=1608.8, opp_rating=1576.1\n",
      "[Ep 10-010] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_001, learner_rating=1591.3, opp_rating=1593.6\n",
      "[Ep 10-011] steps=7, learner_color=0, R_learner=-3.00, score=0.0, opp_id=snap_001, learner_rating=1575.4, opp_rating=1609.5\n",
      "[Ep 10-012] steps=11, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_001, learner_rating=1561.0, opp_rating=1623.9\n",
      "[Ep 10-013] steps=9, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_001, learner_rating=1579.8, opp_rating=1605.0\n",
      "[Ep 10-014] steps=7, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1565.0, opp_rating=1619.9\n",
      "[Ep 10-015] steps=5, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_001, learner_rating=1583.5, opp_rating=1601.4\n",
      "[Ep 10-016] steps=10, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_001, learner_rating=1568.3, opp_rating=1616.5\n",
      "[Ep 10-017] steps=6, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_001, learner_rating=1586.5, opp_rating=1598.3\n",
      "[Ep 10-018] steps=12, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_001, learner_rating=1603.1, opp_rating=1581.8\n",
      "[Ep 10-019] steps=5, learner_color=0, R_learner=-3.00, score=0.0, opp_id=snap_001, learner_rating=1586.1, opp_rating=1598.8\n",
      "[Ep 10-020] steps=17, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_001, learner_rating=1570.7, opp_rating=1614.2\n",
      "[Ep 10-021] steps=9, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_001, learner_rating=1588.7, opp_rating=1596.2\n",
      "[Ep 10-022] steps=13, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_001, learner_rating=1605.0, opp_rating=1579.9\n",
      "[Ep 10-023] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_001, learner_rating=1587.9, opp_rating=1597.0\n",
      "[Ep 10-024] steps=5, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_001, learner_rating=1604.3, opp_rating=1580.6\n",
      "[Ep 10-025] steps=7, learner_color=1, R_learner=3.00, score=1.0, opp_id=snap_001, learner_rating=1619.2, opp_rating=1565.7\n",
      "[Ep 10-026] steps=6, learner_color=1, R_learner=-3.00, score=0.0, opp_id=snap_001, learner_rating=1600.8, opp_rating=1584.1\n",
      "[Ep 10-027] steps=5, learner_color=0, R_learner=-3.00, score=0.0, opp_id=snap_001, learner_rating=1584.0, opp_rating=1600.9\n",
      "[Ep 10-028] steps=8, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1568.8, opp_rating=1616.1\n",
      "[Ep 10-029] steps=6, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1554.9, opp_rating=1629.9\n",
      "[Ep 10-030] steps=11, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_001, learner_rating=1542.3, opp_rating=1642.5\n",
      "[Ep 10-031] steps=9, learner_color=0, R_learner=-3.00, score=0.0, opp_id=snap_003, learner_rating=1522.6, opp_rating=1478.8\n",
      "[Ep 10-032] steps=11, learner_color=0, R_learner=-3.00, score=0.0, opp_id=snap_003, learner_rating=1504.6, opp_rating=1496.8\n",
      "[Ep 10-033] steps=10, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_003, learner_rating=1520.2, opp_rating=1481.1\n",
      "[Ep 10-034] steps=5, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_003, learner_rating=1502.4, opp_rating=1498.9\n",
      "[Ep 10-035] steps=5, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_003, learner_rating=1518.2, opp_rating=1483.1\n",
      "[Ep 10-036] steps=12, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_003, learner_rating=1500.6, opp_rating=1500.7\n",
      "[Ep 10-037] steps=5, learner_color=0, R_learner=-3.00, score=0.0, opp_id=snap_003, learner_rating=1484.6, opp_rating=1516.7\n",
      "[Ep 10-038] steps=7, learner_color=0, R_learner=-1.00, score=0.0, opp_id=snap_003, learner_rating=1470.1, opp_rating=1531.2\n",
      "[Ep 10-039] steps=12, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1484.2, opp_rating=1414.1\n",
      "[Ep 10-040] steps=7, learner_color=0, R_learner=-2.00, score=0.0, opp_id=snap_003, learner_rating=1470.3, opp_rating=1545.1\n",
      "[Ep 10-041] steps=12, learner_color=0, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1483.8, opp_rating=1400.7\n",
      "[Ep 10-042] steps=7, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_003, learner_rating=1502.6, opp_rating=1526.3\n",
      "[Ep 10-043] steps=5, learner_color=0, R_learner=-3.00, score=0.0, opp_id=snap_003, learner_rating=1487.7, opp_rating=1541.2\n",
      "[Ep 10-044] steps=6, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_003, learner_rating=1474.1, opp_rating=1554.7\n",
      "[Ep 10-045] steps=8, learner_color=0, R_learner=2.00, score=1.0, opp_id=snap_000, learner_rating=1486.8, opp_rating=1388.0\n",
      "[Ep 10-046] steps=9, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_003, learner_rating=1505.9, opp_rating=1535.6\n",
      "[Ep 10-047] steps=13, learner_color=1, R_learner=2.00, score=1.0, opp_id=snap_003, learner_rating=1523.2, opp_rating=1518.3\n",
      "[Ep 10-048] steps=12, learner_color=1, R_learner=-2.00, score=0.0, opp_id=snap_003, learner_rating=1507.0, opp_rating=1534.5\n",
      "[Ep 10-049] steps=11, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_003, learner_rating=1524.3, opp_rating=1517.2\n",
      "[Ep 10-050] steps=12, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_003, learner_rating=1507.9, opp_rating=1533.6\n",
      "[Epoch 10] PPO UPDATE: total transitions = 213\n",
      "[Epoch 10] New snapshot added: snap_005 (rating=1507.9)\n",
      "[TRAIN DONE] Saved learner policy to 'shared_policy.pt'\n",
      "\n",
      "=== Final League Ratings ===\n",
      "id=learner, rating=1507.9, games=500\n",
      "id=snap_000, rating=1388.0, games=202\n",
      "id=snap_001, rating=1642.5, games=77\n",
      "id=snap_002, rating=1294.5, games=169\n",
      "id=snap_003, rating=1533.6, games=46\n",
      "id=snap_004, rating=1303.7, games=6\n",
      "id=snap_005, rating=1507.9, games=0\n"
     ]
    }
   ],
   "source": [
    "# agent.py\n",
    "import gymnasium as gym\n",
    "import kymnasium as kym  # env \n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Any, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "#  \n",
    "# ------------------------------------------------\n",
    "N_STONES = 3\n",
    "N_OBS = 3\n",
    "\n",
    "BOARD_W = 600\n",
    "BOARD_H = 600\n",
    "\n",
    "# ---  :   ---\n",
    "N_ANGLES = 16   #  16 (22.5 step)\n",
    "N_POWERS = 4    #  4\n",
    "N_ACTIONS = N_STONES * N_ANGLES * N_POWERS  #    \n",
    "\n",
    "# ------------------------------------------------\n",
    "#  \n",
    "# ------------------------------------------------\n",
    "def make_env(render_mode=None, bgm: bool = False):\n",
    "    env = gym.make(\n",
    "        \"kymnasium/AlKkaGi-3x3-v0\",\n",
    "        obs_type=\"custom\",\n",
    "        render_mode=render_mode,\n",
    "        bgm=bgm,\n",
    "    )\n",
    "    return env\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 1)   \n",
    "# ------------------------------------------------\n",
    "def split_me_opp(obs, my_color: int):\n",
    "    \"\"\"\n",
    "    obs: env  dict\n",
    "      - obs[\"black\"] : (3, 3) = [x, y, alive]\n",
    "      - obs[\"white\"] : (3, 3)\n",
    "      - obs[\"obstacles\"]: (3, 4) = [x, y, w, h]\n",
    "      - obs[\"turn\"]  : 0 ( ) or 1 ( )\n",
    "\n",
    "    my_color: 0=, 1=\n",
    "\n",
    "    return:\n",
    "      me        :   (3, 3)\n",
    "      opp       :   (3, 3)\n",
    "      obstacles :  (3, 4)\n",
    "      turn      : float(0.0 or 1.0)  (env  )\n",
    "    \"\"\"\n",
    "    if my_color == 0:  #  \n",
    "        me = np.array(obs[\"black\"], dtype=np.float32)\n",
    "        opp = np.array(obs[\"white\"], dtype=np.float32)\n",
    "    else:              #  \n",
    "        me = np.array(obs[\"white\"], dtype=np.float32)\n",
    "        opp = np.array(obs[\"black\"], dtype=np.float32)\n",
    "\n",
    "    obstacles = np.array(obs[\"obstacles\"], dtype=np.float32)\n",
    "    turn = float(obs[\"turn\"])\n",
    "    return me, opp, obstacles, turn\n",
    "\n",
    "\n",
    "def normalize_stones(stones, board_w, board_h):\n",
    "    \"\"\"\n",
    "    stones: (N_STONES, 3) = [x, y, alive]\n",
    "    x, y [0,1]  + clip\n",
    "    \"\"\"\n",
    "    out = stones.copy()\n",
    "    out[:, 0] /= board_w  # x\n",
    "    out[:, 1] /= board_h  # y\n",
    "    out[:, 0] = np.clip(out[:, 0], 0.0, 1.0)\n",
    "    out[:, 1] = np.clip(out[:, 1], 0.0, 1.0)\n",
    "    # alive \n",
    "    return out\n",
    "\n",
    "\n",
    "def normalize_obstacles(obs_arr, board_w, board_h):\n",
    "    \"\"\"\n",
    "    obs_arr: (N_OBS, 4) = [x, y, w, h]\n",
    "    x, y, w, h [0,1]    + clip\n",
    "    \"\"\"\n",
    "    out = obs_arr.copy()\n",
    "    out[:, 0] /= board_w  # x\n",
    "    out[:, 1] /= board_h  # y\n",
    "    out[:, 2] /= board_w  # w\n",
    "    out[:, 3] /= board_h  # h\n",
    "\n",
    "    out[:, 0] = np.clip(out[:, 0], 0.0, 1.0)\n",
    "    out[:, 1] = np.clip(out[:, 1], 0.0, 1.0)\n",
    "    out[:, 2] = np.clip(out[:, 2], 0.0, 1.0)\n",
    "    out[:, 3] = np.clip(out[:, 3], 0.0, 1.0)\n",
    "    return out\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 2) Baseline  (31)\n",
    "# ------------------------------------------------\n",
    "def encode_state_basic_alkkagi(\n",
    "    obs,\n",
    "    my_color: int,\n",
    "    board_w: float,\n",
    "    board_h: float,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Baseline  \n",
    "    - turn (1)\n",
    "    - me_norm  : 3 * (x, y, alive) = 9\n",
    "    - opp_norm : 3 * (x, y, alive) = 9\n",
    "    - obs_norm : 3 * (x, y, w, h) = 12\n",
    "\n",
    "    =>  shape: (31,)\n",
    "    \"\"\"\n",
    "    me, opp, obstacles, turn = split_me_opp(obs, my_color)\n",
    "\n",
    "    me_norm  = normalize_stones(me,  board_w, board_h)           # (3, 3)\n",
    "    opp_norm = normalize_stones(opp, board_w, board_h)           # (3, 3)\n",
    "    obs_norm = normalize_obstacles(obstacles, board_w, board_h)  # (3, 4)\n",
    "\n",
    "    feat = np.concatenate([\n",
    "        np.array([turn], dtype=np.float32),  # (1,)\n",
    "        me_norm.flatten(),                   # (9,)\n",
    "        opp_norm.flatten(),                  # (9,)\n",
    "        obs_norm.flatten(),                  # (12,)\n",
    "    ]).astype(np.float32)\n",
    "\n",
    "    # assert feat.shape == (31,)\n",
    "    return feat\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 3) Feature Engineering helper\n",
    "# ------------------------------------------------\n",
    "def group_stats(stones_norm: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    stones_norm: (N_STONES, 3) = [x_norm, y_norm, alive]\n",
    "    alive  :\n",
    "      - center_x, center_y\n",
    "      - var_x, var_y\n",
    "     shape: (4,)\n",
    "    \"\"\"\n",
    "    alive_mask = stones_norm[:, 2] > 0.5\n",
    "    if not np.any(alive_mask):\n",
    "        return np.zeros(4, dtype=np.float32)\n",
    "\n",
    "    xs = stones_norm[alive_mask, 0]\n",
    "    ys = stones_norm[alive_mask, 1]\n",
    "\n",
    "    cx = xs.mean()\n",
    "    cy = ys.mean()\n",
    "    var_x = xs.var()\n",
    "    var_y = ys.var()\n",
    "\n",
    "    return np.array([cx, cy, var_x, var_y], dtype=np.float32)\n",
    "\n",
    "\n",
    "def min_edge_dist(stones_norm: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    alive      .\n",
    "     [0,1]  .\n",
    "    \"\"\"\n",
    "    alive_mask = stones_norm[:, 2] > 0.5\n",
    "    if not np.any(alive_mask):\n",
    "        return 0.0\n",
    "\n",
    "    xs = stones_norm[alive_mask, 0]\n",
    "    ys = stones_norm[alive_mask, 1]\n",
    "\n",
    "    edge_dists = np.minimum.reduce([xs, 1.0 - xs, ys, 1.0 - ys])\n",
    "    return float(edge_dists.min())\n",
    "\n",
    "\n",
    "def min_pairwise_dist(me_norm: np.ndarray, opp_norm: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    alive   vs alive       .\n",
    "    \"\"\"\n",
    "    me_alive = me_norm[me_norm[:, 2] > 0.5]\n",
    "    opp_alive = opp_norm[opp_norm[:, 2] > 0.5]\n",
    "\n",
    "    if me_alive.size == 0 or opp_alive.size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    min_d = 1e9\n",
    "    for a in me_alive:\n",
    "        for b in opp_alive:\n",
    "            dx = a[0] - b[0]\n",
    "            dy = a[1] - b[1]\n",
    "            d = np.sqrt(dx * dx + dy * dy)\n",
    "            if d < min_d:\n",
    "                min_d = d\n",
    "\n",
    "    return float(min_d)\n",
    "\n",
    "\n",
    "def obstacle_summary(obs_norm: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    obs_norm: (N_OBS, 4) = [x_norm, y_norm, w_norm, h_norm]\n",
    "\n",
    "    - count\n",
    "    - center_x_mean, center_y_mean\n",
    "    - w_mean, h_mean\n",
    "\n",
    "     shape: (5,)\n",
    "    \"\"\"\n",
    "    if obs_norm.size == 0:\n",
    "        return np.zeros(5, dtype=np.float32)\n",
    "\n",
    "    cx = obs_norm[:, 0] + obs_norm[:, 2] / 2.0\n",
    "    cy = obs_norm[:, 1] + obs_norm[:, 3] / 2.0\n",
    "    w = obs_norm[:, 2]\n",
    "    h = obs_norm[:, 3]\n",
    "\n",
    "    cnt = float(obs_norm.shape[0])\n",
    "    return np.array(\n",
    "        [cnt, cx.mean(), cy.mean(), w.mean(), h.mean()],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4) Feature Engineering  (52)\n",
    "# ------------------------------------------------\n",
    "def encode_state_fe_alkkagi(\n",
    "    obs,\n",
    "    my_color: int,\n",
    "    board_w: float,\n",
    "    board_h: float,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Feature engineering  state encoder.\n",
    "\n",
    "    - baseline feature: 31\n",
    "    -  feature:\n",
    "        * turn_is_me, my_alive_cnt, opp_alive_cnt, alive_diff, alive_ratio (5)\n",
    "        * my_center_x, my_center_y, my_var_x, my_var_y (4)\n",
    "        * op_center_x, op_center_y, op_var_x, op_var_y (4)\n",
    "        * my_min_edge, op_min_edge, min_my_op_dist (3)\n",
    "        * obs_cnt, obs_cx_mean, obs_cy_mean, obs_w_mean, obs_h_mean (5)\n",
    "\n",
    "      =>  21\n",
    "      =>  31 + 21 = 52\n",
    "    \"\"\"\n",
    "    # --- 1) baseline feature (31) ---\n",
    "    base_feat = encode_state_basic_alkkagi(obs, my_color, board_w, board_h)\n",
    "\n",
    "    # --- 2)  /   ---\n",
    "    me, opp, obstacles, turn_raw = split_me_opp(obs, my_color)\n",
    "    me_norm  = normalize_stones(me,  board_w, board_h)\n",
    "    opp_norm = normalize_stones(opp, board_w, board_h)\n",
    "    obs_norm = normalize_obstacles(obstacles, board_w, board_h)\n",
    "\n",
    "    # --- 3) scalar feature ---\n",
    "    my_alive_cnt  = float((me_norm[:, 2] > 0.5).sum())\n",
    "    opp_alive_cnt = float((opp_norm[:, 2] > 0.5).sum())\n",
    "    alive_diff    = my_alive_cnt - opp_alive_cnt\n",
    "    denom = my_alive_cnt + opp_alive_cnt\n",
    "    alive_ratio = my_alive_cnt / denom if denom > 0 else 0.0\n",
    "\n",
    "    # env  (0=,1=) my_color \n",
    "    turn_is_me = 1.0 if int(turn_raw) == int(my_color) else 0.0\n",
    "\n",
    "    scalar_feats = np.array(\n",
    "        [turn_is_me, my_alive_cnt, opp_alive_cnt, alive_diff, alive_ratio],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "\n",
    "    # --- 4)   ( + ) ---\n",
    "    my_stats = group_stats(me_norm)   # (4,)\n",
    "    op_stats = group_stats(opp_norm)  # (4,)\n",
    "\n",
    "    # --- 5)  feature ---\n",
    "    my_min_edge = min_edge_dist(me_norm)\n",
    "    op_min_edge = min_edge_dist(opp_norm)\n",
    "    min_my_op   = min_pairwise_dist(me_norm, opp_norm)\n",
    "\n",
    "    relation_feats = np.array(\n",
    "        [my_min_edge, op_min_edge, min_my_op],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "\n",
    "    # --- 6)   ---\n",
    "    obs_stats = obstacle_summary(obs_norm)  # (5,)\n",
    "\n",
    "    # --- 7)  concat ---\n",
    "    extra_feats = np.concatenate([\n",
    "        scalar_feats,   # 5\n",
    "        my_stats,       # 4\n",
    "        op_stats,       # 4\n",
    "        relation_feats, # 3\n",
    "        obs_stats,      # 5\n",
    "    ]).astype(np.float32)\n",
    "\n",
    "    feat = np.concatenate([base_feat, extra_feats]).astype(np.float32)\n",
    "\n",
    "    # assert feat.shape == (52,)\n",
    "    return feat\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 5) torch  \n",
    "# ------------------------------------------------\n",
    "def encode_state_basic_tensor(\n",
    "    obs,\n",
    "    my_color: int,\n",
    "    device: torch.device | None = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    basic encoder  torch.Tensor\n",
    "    shape: (31,)     (1, 31)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    feat_np = encode_state_basic_alkkagi(obs, my_color, BOARD_W, BOARD_H)\n",
    "    feat_t = torch.from_numpy(feat_np).to(device=device, dtype=torch.float32)\n",
    "    return feat_t  #    unsqueeze(0)  \n",
    "\n",
    "\n",
    "def encode_state_fe_tensor(\n",
    "    obs,\n",
    "    my_color: int,\n",
    "    device: torch.device | None = None,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    feature engineering encoder  torch.Tensor\n",
    "    shape: (52,)     (1, 52)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    feat_np = encode_state_fe_alkkagi(obs, my_color, BOARD_W, BOARD_H)\n",
    "    feat_t = torch.from_numpy(feat_np).to(device=device, dtype=torch.float32)\n",
    "    return feat_t\n",
    "\n",
    "\n",
    "def decode_action_index(action_idx: int):\n",
    "    \"\"\"\n",
    "       -> (stone_id, angle, power) .\n",
    "\n",
    "    - stone_id  {0,1,2}\n",
    "    - angle  [-180, 180]   \n",
    "    - power  [500, 2500]   \n",
    "    \"\"\"\n",
    "    #    (stone, angle, power) 3  \n",
    "    per_stone = N_ANGLES * N_POWERS\n",
    "    stone_id = action_idx // per_stone\n",
    "    rem = action_idx % per_stone\n",
    "\n",
    "    angle_id = rem // N_POWERS\n",
    "    power_id = rem % N_POWERS\n",
    "\n",
    "    # angle \n",
    "    angle_step = 360.0 / N_ANGLES\n",
    "    angle = -180.0 + (angle_id + 0.5) * angle_step  # \n",
    "\n",
    "    # power \n",
    "    power_min, power_max = 500.0, 2500.0\n",
    "    if N_POWERS == 1:\n",
    "        power = (power_min + power_max) / 2.0\n",
    "    else:\n",
    "        ratio = power_id / (N_POWERS - 1)\n",
    "        power = power_min + ratio * (power_max - power_min)\n",
    "\n",
    "    return int(stone_id), float(angle), float(power)\n",
    "\n",
    "\n",
    "def save_checkpoint_policy(\n",
    "    policy: PPOPolicy,\n",
    "    epoch: int,\n",
    "    checkpoint_dir: str = \"checkpoints\",\n",
    "    max_keep: int = 20,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    policy  checkpoint_dir ,\n",
    "     max_keep   .\n",
    "    \"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    ckpt_path = os.path.join(checkpoint_dir, f\"policy_epoch_{epoch:06d}.pt\")\n",
    "    policy.save(ckpt_path)\n",
    "\n",
    "    #   \n",
    "    ckpts = sorted(Path(checkpoint_dir).glob(\"policy_epoch_*.pt\"))\n",
    "    if len(ckpts) > max_keep:\n",
    "        for old in ckpts[:-max_keep]:\n",
    "            try:\n",
    "                old.unlink()\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "    return ckpt_path\n",
    "\n",
    "\n",
    "def append_epoch_log(\n",
    "    epoch: int,\n",
    "    episodes: int,\n",
    "    wins: int,\n",
    "    draws: int,\n",
    "    losses: int,\n",
    "    avg_reward: float,\n",
    "    avg_steps: float,\n",
    "    learner_rating: float,\n",
    "    num_players: int,\n",
    "    log_path: str = \"training_metrics.csv\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "         CSV .\n",
    "        ,   append.\n",
    "    \"\"\"\n",
    "    file_exists = os.path.exists(log_path)\n",
    "\n",
    "    #   \n",
    "    if not file_exists:\n",
    "        with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\n",
    "                \"epoch,episodes,wins,draws,losses,win_rate,avg_reward,avg_steps,learner_rating,num_players\\n\"\n",
    "            )\n",
    "\n",
    "    win_rate = wins / episodes if episodes > 0 else 0.0\n",
    "\n",
    "    with open(log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\n",
    "            f\"{epoch},{episodes},{wins},{draws},{losses},\"\n",
    "            f\"{win_rate:.6f},{avg_reward:.6f},{avg_steps:.6f},\"\n",
    "            f\"{learner_rating:.6f},{num_players}\\n\"\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 6) Policy / Value Network\n",
    "# ------------------------------------------------\n",
    "class PolicyValueNet(nn.Module):\n",
    "    \"\"\"\n",
    "    52 state -> (logits) + (value)   MLP.\n",
    "     PPO    .\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim: int = 52, n_actions: int = N_ACTIONS):\n",
    "        super().__init__()\n",
    "        hidden = 128\n",
    "        self.fc1 = nn.Linear(state_dim, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.policy_head = nn.Linear(hidden, n_actions)\n",
    "        self.value_head = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (B, state_dim)\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        logits = self.policy_head(h)              # (B, n_actions)\n",
    "        value = self.value_head(h).squeeze(-1)    # (B,)\n",
    "        return logits, value\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PPOConfig:\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    clip_coef: float = 0.2\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 0.5\n",
    "    learning_rate: float = 3e-4\n",
    "    update_epochs: int = 4\n",
    "    batch_size: int = 64\n",
    "\n",
    "\n",
    "def compute_terminal_reward(obs, my_color: int) -> float:\n",
    "    \"\"\"\n",
    "      ,  \n",
    "    my_color(0=,1=)  alive_diff  .\n",
    "    \"\"\"\n",
    "    me, opp, _, _ = split_me_opp(obs, my_color)\n",
    "    me = np.array(me, dtype=np.float32)\n",
    "    opp = np.array(opp, dtype=np.float32)\n",
    "\n",
    "    my_alive = float((me[:, 2] > 0.5).sum())\n",
    "    opp_alive = float((opp[:, 2] > 0.5).sum())\n",
    "    alive_diff = my_alive - opp_alive  # [-3, +3] \n",
    "\n",
    "    return alive_diff\n",
    "\n",
    "\n",
    "def compute_gae_returns(\n",
    "    rewards: np.ndarray,\n",
    "    values: np.ndarray,\n",
    "    gamma: float,\n",
    "    lam: float,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "       GAE + return .\n",
    "    rewards: (T,)\n",
    "    values : (T,)\n",
    "    \"\"\"\n",
    "    T = len(rewards)\n",
    "    advantages = np.zeros(T, dtype=np.float32)\n",
    "    returns = np.zeros(T, dtype=np.float32)\n",
    "\n",
    "    gae = 0.0\n",
    "    next_value = 0.0  # terminal bootstrap \n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        delta = rewards[t] + gamma * next_value - values[t]\n",
    "        gae = delta + gamma * lam * gae\n",
    "        advantages[t] = gae\n",
    "        next_value = values[t]\n",
    "        returns[t] = advantages[t] + values[t]\n",
    "\n",
    "    return advantages, returns\n",
    "\n",
    "\n",
    "\n",
    "def clone_policy(src: PPOPolicy, new_lr: float | None = None) -> PPOPolicy:\n",
    "    \"\"\"\n",
    "      learner policy  \n",
    "    opponent snapshot   helper.\n",
    "\n",
    "    - (state_dict) \n",
    "    - optimizer  \n",
    "    \"\"\"\n",
    "    device = src.device\n",
    "    if new_lr is None:\n",
    "        #  optimizer lr  (param_group 0 )\n",
    "        lr = src.optimizer.param_groups[0][\"lr\"]\n",
    "    else:\n",
    "        lr = new_lr\n",
    "\n",
    "    new_policy = PPOPolicy(device=device, lr=lr)\n",
    "    new_policy.model.load_state_dict(src.model.state_dict())\n",
    "    return new_policy\n",
    "\n",
    "\n",
    "\n",
    "class PPOPolicy:\n",
    "    \"\"\"\n",
    "     PolicyValueNet /   PPO .\n",
    "    - act_eval :  (no-grad)\n",
    "    - act_train:  (logprob, value, state  )\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, device: torch.device | None = None, lr: float = 3e-4):\n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = device\n",
    "\n",
    "        self.model = PolicyValueNet(state_dim=52, n_actions=N_ACTIONS).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "    # =====   =====\n",
    "    def save(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "          .\n",
    "        \"\"\"\n",
    "        ckpt = {\n",
    "            \"state_dict\": self.model.state_dict(),\n",
    "        }\n",
    "        torch.save(ckpt, path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(\n",
    "        cls,\n",
    "        path: str,\n",
    "        device: torch.device | None = None,\n",
    "        lr: float = 3e-4,\n",
    "    ) -> \"PPOPolicy\":\n",
    "        \"\"\"\n",
    "           PPOPolicy   .\n",
    "        \"\"\"\n",
    "        policy = cls(device=device, lr=lr)\n",
    "        ckpt = torch.load(path, map_location=policy.device)\n",
    "\n",
    "        if isinstance(ckpt, dict) and \"state_dict\" in ckpt:\n",
    "            state_dict = ckpt[\"state_dict\"]\n",
    "        else:\n",
    "            state_dict = ckpt\n",
    "\n",
    "        policy.model.load_state_dict(state_dict)\n",
    "        return policy\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act_eval(self, observation: Dict[str, Any], my_color: int) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        / act ( or    ).\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        state_t = encode_state_fe_tensor(\n",
    "            observation,\n",
    "            my_color=my_color,\n",
    "            device=self.device,\n",
    "        ).unsqueeze(0)  # (1, 52)\n",
    "\n",
    "        logits, value = self.model(state_t)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action_idx = dist.sample().item()\n",
    "\n",
    "        stone_id, angle, power = decode_action_index(action_idx)\n",
    "\n",
    "        #       \n",
    "        if my_color == 0:\n",
    "            stones = observation[\"black\"]\n",
    "        else:\n",
    "            stones = observation[\"white\"]\n",
    "\n",
    "        n_stones = len(stones)\n",
    "        if n_stones > 0:\n",
    "            stone_id = stone_id % n_stones\n",
    "        else:\n",
    "            stone_id = 0\n",
    "\n",
    "        action = {\n",
    "            \"turn\": int(observation[\"turn\"]),\n",
    "            \"index\": int(stone_id),\n",
    "            \"power\": float(power),\n",
    "            \"angle\": float(angle),\n",
    "        }\n",
    "        return action\n",
    "\n",
    "    def act_train(\n",
    "        self,\n",
    "        observation: Dict[str, Any],\n",
    "        my_color: int,\n",
    "    ) -> tuple[Dict[str, Any], int, float, float, torch.Tensor]:\n",
    "        \"\"\"\n",
    "         act.\n",
    "        :\n",
    "          - action dict\n",
    "          - action_idx (int)\n",
    "          - logprob (float)\n",
    "          - value (float)\n",
    "          - state_tensor (52,)   rollout \n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "\n",
    "        state_t = encode_state_fe_tensor(\n",
    "            observation,\n",
    "            my_color=my_color,\n",
    "            device=self.device,\n",
    "        ).unsqueeze(0)  # (1, 52)\n",
    "\n",
    "        logits, value_t = self.model(state_t)          # logits: (1, A), value_t: (1,1)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action_idx_t = dist.sample()                   # (1,)\n",
    "        logprob_t = dist.log_prob(action_idx_t)        # (1,)\n",
    "\n",
    "        action_idx = int(action_idx_t.item())\n",
    "        logprob = float(logprob_t.item())\n",
    "        value = float(value_t.squeeze(0).item())\n",
    "\n",
    "        stone_id, angle, power = decode_action_index(action_idx)\n",
    "\n",
    "        if my_color == 0:\n",
    "            stones = observation[\"black\"]\n",
    "        else:\n",
    "            stones = observation[\"white\"]\n",
    "\n",
    "        n_stones = len(stones)\n",
    "        if n_stones > 0:\n",
    "            stone_id = stone_id % n_stones\n",
    "        else:\n",
    "            stone_id = 0\n",
    "\n",
    "        action = {\n",
    "            \"turn\": int(observation[\"turn\"]),\n",
    "            \"index\": int(stone_id),\n",
    "            \"power\": float(power),\n",
    "            \"angle\": float(angle),\n",
    "        }\n",
    "\n",
    "        # (52,)   (  )\n",
    "        state_vec = state_t.squeeze(0).detach()  # (52,)\n",
    "\n",
    "        return action, action_idx, logprob, value, state_vec\n",
    "\n",
    "\n",
    "def ppo_update(\n",
    "    policy: PPOPolicy,\n",
    "    states: list[torch.Tensor],\n",
    "    actions: list[int],\n",
    "    old_logprobs: list[float],\n",
    "    advantages: list[float],\n",
    "    returns: list[float],\n",
    "    config: PPOConfig,\n",
    "):\n",
    "    device = policy.device\n",
    "    policy.model.train()\n",
    "\n",
    "    states_t = torch.stack(states).to(device)  # (N, 52)\n",
    "    actions_t = torch.tensor(actions, dtype=torch.long, device=device)             # (N,)\n",
    "    old_logprobs_t = torch.tensor(old_logprobs, dtype=torch.float32, device=device)  # (N,)\n",
    "    advantages_t = torch.tensor(advantages, dtype=torch.float32, device=device)      # (N,)\n",
    "    returns_t = torch.tensor(returns, dtype=torch.float32, device=device)            # (N,)\n",
    "\n",
    "    # advantage \n",
    "    advantages_t = (advantages_t - advantages_t.mean()) / (advantages_t.std() + 1e-8)\n",
    "\n",
    "    N = states_t.size(0)\n",
    "    batch_size = min(config.batch_size, N)\n",
    "\n",
    "    for _ in range(config.update_epochs):\n",
    "        idx = torch.randperm(N, device=device)\n",
    "\n",
    "        for start in range(0, N, batch_size):\n",
    "            mb_idx = idx[start:start + batch_size]\n",
    "\n",
    "            mb_states = states_t[mb_idx]\n",
    "            mb_actions = actions_t[mb_idx]\n",
    "            mb_old_logprobs = old_logprobs_t[mb_idx]\n",
    "            mb_adv = advantages_t[mb_idx]\n",
    "            mb_returns = returns_t[mb_idx]\n",
    "\n",
    "            logits, values = policy.model(mb_states)      # logits: (B, A), values: (B,)\n",
    "            dist = torch.distributions.Categorical(logits=logits)\n",
    "            new_logprobs = dist.log_prob(mb_actions)      # (B,)\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            ratio = (new_logprobs - mb_old_logprobs).exp()\n",
    "            surr1 = ratio * mb_adv\n",
    "            surr2 = torch.clamp(\n",
    "                ratio,\n",
    "                1.0 - config.clip_coef,\n",
    "                1.0 + config.clip_coef,\n",
    "            ) * mb_adv\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            value_loss = F.mse_loss(values, mb_returns)\n",
    "\n",
    "            loss = policy_loss + config.vf_coef * value_loss - config.ent_coef * entropy\n",
    "\n",
    "            policy.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(policy.model.parameters(), config.max_grad_norm)\n",
    "            policy.optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_selfplay_shared_ppo(\n",
    "    num_episodes: int = 50,\n",
    "    config: PPOConfig | None = None,\n",
    ") -> PPOPolicy:\n",
    "    \"\"\"\n",
    "    /   PPOPolicy  self-play .\n",
    "    -  env reset\n",
    "    - obs['turn'] == 0    act_train\n",
    "    - obs['turn'] == 1    act_train\n",
    "    - :\n",
    "        R_black = compute_terminal_reward(obs_final, my_color=0)\n",
    "        R_white = compute_terminal_reward(obs_final, my_color=1)\n",
    "           \n",
    "    - / rollout    PPO \n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = PPOConfig()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    env = make_env(render_mode=None, bgm=False)\n",
    "\n",
    "    policy = PPOPolicy(device=device, lr=config.learning_rate)\n",
    "\n",
    "    all_states: list[torch.Tensor] = []\n",
    "    all_actions: list[int] = []\n",
    "    all_logprobs: list[float] = []\n",
    "    all_advantages: list[float] = []\n",
    "    all_returns: list[float] = []\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        obs, info = env.reset(seed=ep)\n",
    "        done = False\n",
    "        step = 0\n",
    "\n",
    "        #  rollout \n",
    "        states_black: list[torch.Tensor] = []\n",
    "        actions_black: list[int] = []\n",
    "        logprobs_black: list[float] = []\n",
    "        values_black: list[float] = []\n",
    "        rewards_black: list[float] = []\n",
    "\n",
    "        states_white: list[torch.Tensor] = []\n",
    "        actions_white: list[int] = []\n",
    "        logprobs_white: list[float] = []\n",
    "        values_white: list[float] = []\n",
    "        rewards_white: list[float] = []\n",
    "\n",
    "        while not done:\n",
    "            turn = int(obs[\"turn\"])  # 0=, 1=\n",
    "\n",
    "            #       +  \n",
    "            action, action_idx, logprob, value, state_vec = policy.act_train(\n",
    "                observation=obs,\n",
    "                my_color=turn,\n",
    "            )\n",
    "\n",
    "            #  rollout \n",
    "            if turn == 0:\n",
    "                states_black.append(state_vec.cpu())\n",
    "                actions_black.append(action_idx)\n",
    "                logprobs_black.append(logprob)\n",
    "                values_black.append(value)\n",
    "                rewards_black.append(0.0)  #  0,    \n",
    "            else:\n",
    "                states_white.append(state_vec.cpu())\n",
    "                actions_white.append(action_idx)\n",
    "                logprobs_white.append(logprob)\n",
    "                values_white.append(value)\n",
    "                rewards_white.append(0.0)\n",
    "\n",
    "            #  \n",
    "            obs, reward_env, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            step += 1\n",
    "\n",
    "        #   ,      \n",
    "        R_black = compute_terminal_reward(obs, my_color=0)\n",
    "        R_white = compute_terminal_reward(obs, my_color=1)  #  -R_black\n",
    "\n",
    "        if len(rewards_black) > 0:\n",
    "            rewards_black[-1] += R_black\n",
    "        if len(rewards_white) > 0:\n",
    "            rewards_white[-1] += R_white\n",
    "\n",
    "        #  GAE + return  ,   \n",
    "        if len(rewards_black) > 0:\n",
    "            r_b = np.array(rewards_black, dtype=np.float32)\n",
    "            v_b = np.array(values_black, dtype=np.float32)\n",
    "            adv_b, ret_b = compute_gae_returns(\n",
    "                rewards=r_b,\n",
    "                values=v_b,\n",
    "                gamma=config.gamma,\n",
    "                lam=config.gae_lambda,\n",
    "            )\n",
    "\n",
    "            all_states.extend(states_black)\n",
    "            all_actions.extend(actions_black)\n",
    "            all_logprobs.extend(logprobs_black)\n",
    "            all_advantages.extend(adv_b.tolist())\n",
    "            all_returns.extend(ret_b.tolist())\n",
    "\n",
    "        if len(rewards_white) > 0:\n",
    "            r_w = np.array(rewards_white, dtype=np.float32)\n",
    "            v_w = np.array(values_white, dtype=np.float32)\n",
    "            adv_w, ret_w = compute_gae_returns(\n",
    "                rewards=r_w,\n",
    "                values=v_w,\n",
    "                gamma=config.gamma,\n",
    "                lam=config.gae_lambda,\n",
    "            )\n",
    "\n",
    "            all_states.extend(states_white)\n",
    "            all_actions.extend(actions_white)\n",
    "            all_logprobs.extend(logprobs_white)\n",
    "            all_advantages.extend(adv_w.tolist())\n",
    "            all_returns.extend(ret_w.tolist())\n",
    "\n",
    "        print(\n",
    "            f\"[Ep {ep+1:03d}/{num_episodes:03d}] \"\n",
    "            f\"steps={step}, R_black={R_black:.2f}, R_white={R_white:.2f}, \"\n",
    "            f\"transitions_B={len(rewards_black)}, W={len(rewards_white)}\"\n",
    "        )\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    #   transition PPO  (or  ) \n",
    "    if len(all_states) > 0:\n",
    "        print(f\"\\n[ PPO UPDATE ] total transitions = {len(all_states)}\")\n",
    "        ppo_update(\n",
    "            policy=policy,\n",
    "            states=all_states,\n",
    "            actions=all_actions,\n",
    "            old_logprobs=all_logprobs,\n",
    "            advantages=all_advantages,\n",
    "            returns=all_returns,\n",
    "            config=config,\n",
    "        )\n",
    "    else:\n",
    "        print(\" transition .\")\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class YourBlackAgent(kym.Agent):\n",
    "    def __init__(self, policy: PPOPolicy | None = None, device: torch.device | None = None):\n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = device\n",
    "\n",
    "        if policy is None:\n",
    "            self.policy = PPOPolicy(device=self.device)\n",
    "        else:\n",
    "            self.policy = policy\n",
    "\n",
    "    def act(self, observation, info):\n",
    "        #   my_color=0\n",
    "        return self.policy.act_eval(observation, my_color=0)\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        self.policy.save(path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> \"YourBlackAgent\":\n",
    "        policy = PPOPolicy.load(path)\n",
    "        return cls(policy=policy, device=policy.device)\n",
    "\n",
    "\n",
    "class YourWhiteAgent(kym.Agent):\n",
    "    def __init__(self, policy: PPOPolicy | None = None, device: torch.device | None = None):\n",
    "        if device is None:\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = device\n",
    "\n",
    "        if policy is None:\n",
    "            self.policy = PPOPolicy(device=self.device)\n",
    "        else:\n",
    "            self.policy = policy\n",
    "\n",
    "    def act(self, observation, info):\n",
    "        #   my_color=1\n",
    "        return self.policy.act_eval(observation, my_color=1)\n",
    "\n",
    "    def save(self, path: str) -> None:\n",
    "        self.policy.save(path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> \"YourWhiteAgent\":\n",
    "        policy = PPOPolicy.load(path)\n",
    "        return cls(policy=policy, device=policy.device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RatedPolicy:\n",
    "    \"\"\"\n",
    "    ELO    .\n",
    "    - id    : 'main_v1', 'snapshot_005'  \n",
    "    - policy: PPOPolicy  (  path   )\n",
    "    - rating:  ELO \n",
    "    - games :   \n",
    "    \"\"\"\n",
    "    id: str\n",
    "    policy: PPOPolicy\n",
    "    rating: float = 1500.0\n",
    "    games: int = 0\n",
    "\n",
    "\n",
    "def elo_expected(ra: float, rb: float) -> float:\n",
    "    \"\"\"\n",
    "    E_A = 1 / (1 + 10^((Rb - Ra)/400))\n",
    "    \"\"\"\n",
    "    return 1.0 / (1.0 + 10.0 ** ((rb - ra) / 400.0))\n",
    "\n",
    "\n",
    "def elo_update(\n",
    "    ra: float,\n",
    "    rb: float,\n",
    "    score_a: float,\n",
    "    k: float = 32.0,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    score_a: A   (1=, 0.5=, 0=)\n",
    "    B score 1-score_a .\n",
    "    \"\"\"\n",
    "    ea = elo_expected(ra, rb)\n",
    "    eb = 1.0 - ea\n",
    "\n",
    "    ra_new = ra + k * (score_a - ea)\n",
    "    rb_new = rb + k * ((1.0 - score_a) - eb)\n",
    "    return ra_new, rb_new\n",
    "\n",
    "\n",
    "class EloLeague:\n",
    "    \"\"\"\n",
    "    ELO  self-play   .\n",
    "    - players  learner opponent \n",
    "    -     .\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, players: List[RatedPolicy], k: float = 32.0):\n",
    "        self.players = players\n",
    "        self.k = k\n",
    "\n",
    "    def find_index(self, player_id: str) -> int:\n",
    "        for i, p in enumerate(self.players):\n",
    "            if p.id == player_id:\n",
    "                return i\n",
    "        raise ValueError(f\"player_id '{player_id}' not found\")\n",
    "\n",
    "    def choose_opponent(self, learner_id: str) -> RatedPolicy:\n",
    "        \"\"\"\n",
    "          :\n",
    "        - learner rating     .\n",
    "          /   .\n",
    "        \"\"\"\n",
    "        li = self.find_index(learner_id)\n",
    "        learner = self.players[li]\n",
    "\n",
    "        candidates = [\n",
    "            p for p in self.players\n",
    "            if p.id != learner_id\n",
    "        ]\n",
    "        if not candidates:\n",
    "            raise RuntimeError(\"opponent candidates .\")\n",
    "\n",
    "        # rating    \n",
    "        opponent = min(candidates, key=lambda p: abs(p.rating - learner.rating))\n",
    "        return opponent\n",
    "\n",
    "    def update_result(self, a_id: str, b_id: str, score_a: float) -> None:\n",
    "        \"\"\"\n",
    "          ,  rating .\n",
    "        \"\"\"\n",
    "        ai = self.find_index(a_id)\n",
    "        bi = self.find_index(b_id)\n",
    "        pa = self.players[ai]\n",
    "        pb = self.players[bi]\n",
    "\n",
    "        ra_new, rb_new = elo_update(pa.rating, pb.rating, score_a, k=self.k)\n",
    "\n",
    "        pa.rating = ra_new\n",
    "        pb.rating = rb_new\n",
    "        pa.games += 1\n",
    "        pb.games += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_league_selfplay(\n",
    "    num_epochs: int = 5,\n",
    "    episodes_per_epoch: int = 20,\n",
    "    snapshot_interval: int = 2,\n",
    "    config: PPOConfig | None = None,\n",
    "    checkpoint_dir: str = \"checkpoints\",\n",
    "    checkpoint_interval: int = 5000,\n",
    "    max_checkpoints: int = 20,\n",
    "    log_path: str = \"training_metrics.csv\",\n",
    ") -> tuple[PPOPolicy, EloLeague]:\n",
    "    \"\"\"\n",
    "    ELO   self-play  .\n",
    "\n",
    "    - learner :      (ID: \"learner\")\n",
    "    - opponents:   (ID: \"snap_000\", \"snap_001\", ...)\n",
    "\n",
    "     epoch:\n",
    "      1)    learner vs opponent self-play\n",
    "      2) learner transition PPO \n",
    "      3) snapshot_interval learner  \n",
    "      4) checkpoint_interval checkpoints/ ckpt  ( max_checkpoints )\n",
    "      5) training_metrics.csv    \n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = PPOConfig()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    env = make_env(render_mode=None, bgm=False)\n",
    "\n",
    "    # 1) learner \n",
    "    learner = PPOPolicy(device=device, lr=config.learning_rate)\n",
    "    # 2)  opponent   \n",
    "    snap0 = clone_policy(learner)\n",
    "\n",
    "    # 3) ELO  \n",
    "    league = EloLeague(\n",
    "        players=[\n",
    "            RatedPolicy(id=\"learner\", policy=learner, rating=1500.0),\n",
    "            RatedPolicy(id=\"snap_000\", policy=snap0, rating=1500.0),\n",
    "        ],\n",
    "        k=32.0,\n",
    "    )\n",
    "    snapshot_counter = 1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        all_states: list[torch.Tensor] = []\n",
    "        all_actions: list[int] = []\n",
    "        all_logprobs: list[float] = []\n",
    "        all_advantages: list[float] = []\n",
    "        all_returns: list[float] = []\n",
    "\n",
    "        # ----    ----\n",
    "        ep_wins = 0\n",
    "        ep_draws = 0\n",
    "        ep_losses = 0\n",
    "        ep_reward_sum = 0.0\n",
    "        ep_step_sum = 0\n",
    "        episodes_this_epoch = 0\n",
    "\n",
    "        print(f\"\\n===== [Epoch {epoch+1}/{num_epochs}] =====\")\n",
    "        for ep in range(episodes_per_epoch):\n",
    "            # --- opponent  (learner rating  ) ---\n",
    "            opponent = league.choose_opponent(\"learner\")\n",
    "\n",
    "            # learner   : 0=, 1=\n",
    "            learner_color = int(np.random.randint(0, 2))\n",
    "\n",
    "            # env reset\n",
    "            global_seed = epoch * episodes_per_epoch + ep\n",
    "            obs, info = env.reset(seed=global_seed)\n",
    "            done = False\n",
    "            step = 0\n",
    "\n",
    "            # learner rollout \n",
    "            ep_states: list[torch.Tensor] = []\n",
    "            ep_actions: list[int] = []\n",
    "            ep_logprobs: list[float] = []\n",
    "            ep_values: list[float] = []\n",
    "            ep_rewards: list[float] = []\n",
    "\n",
    "            while not done:\n",
    "                turn = int(obs[\"turn\"])  #    (0 or 1)\n",
    "\n",
    "                if turn == learner_color:\n",
    "                    # ----- learner  ( ) -----\n",
    "                    action, action_idx, logprob, value, state_vec = learner.act_train(\n",
    "                        observation=obs,\n",
    "                        my_color=turn,\n",
    "                    )\n",
    "\n",
    "                    ep_states.append(state_vec.cpu())\n",
    "                    ep_actions.append(action_idx)\n",
    "                    ep_logprobs.append(logprob)\n",
    "                    ep_values.append(value)\n",
    "                    ep_rewards.append(0.0)  #    \n",
    "                else:\n",
    "                    # ----- opponent  (gradient X) -----\n",
    "                    action = opponent.policy.act_eval(\n",
    "                        observation=obs,\n",
    "                        my_color=turn,\n",
    "                    )\n",
    "\n",
    "                obs, reward_env, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                step += 1\n",
    "\n",
    "            # ----  : learner    &   ----\n",
    "            R_learner = compute_terminal_reward(obs, my_color=learner_color)\n",
    "            if len(ep_rewards) > 0:\n",
    "                ep_rewards[-1] += R_learner\n",
    "\n",
    "            #   ELO score (1/0.5/0)\n",
    "            if R_learner > 0:\n",
    "                score_a = 1.0\n",
    "                ep_wins += 1\n",
    "            elif R_learner < 0:\n",
    "                score_a = 0.0\n",
    "                ep_losses += 1\n",
    "            else:\n",
    "                score_a = 0.5\n",
    "                ep_draws += 1\n",
    "\n",
    "            league.update_result(\"learner\", opponent.id, score_a)\n",
    "\n",
    "            #  \n",
    "            episodes_this_epoch += 1\n",
    "            ep_reward_sum += R_learner\n",
    "            ep_step_sum += step\n",
    "\n",
    "            # ---- learner rollout  GAE + return  ----\n",
    "            if len(ep_rewards) > 0:\n",
    "                rewards_np = np.array(ep_rewards, dtype=np.float32)\n",
    "                values_np = np.array(ep_values, dtype=np.float32)\n",
    "                adv_np, ret_np = compute_gae_returns(\n",
    "                    rewards=rewards_np,\n",
    "                    values=values_np,\n",
    "                    gamma=config.gamma,\n",
    "                    lam=config.gae_lambda,\n",
    "                )\n",
    "\n",
    "                all_states.extend(ep_states)\n",
    "                all_actions.extend(ep_actions)\n",
    "                all_logprobs.extend(ep_logprobs)\n",
    "                all_advantages.extend(adv_np.tolist())\n",
    "                all_returns.extend(ret_np.tolist())\n",
    "\n",
    "            learner_idx = league.find_index(\"learner\")\n",
    "            learner_rating = league.players[learner_idx].rating\n",
    "\n",
    "            print(\n",
    "                f\"[Ep {epoch+1:02d}-{ep+1:03d}] \"\n",
    "                f\"steps={step}, learner_color={learner_color}, \"\n",
    "                f\"R_learner={R_learner:.2f}, score={score_a:.1f}, \"\n",
    "                f\"opp_id={opponent.id}, \"\n",
    "                f\"learner_rating={learner_rating:.1f}, \"\n",
    "                f\"opp_rating={opponent.rating:.1f}\"\n",
    "            )\n",
    "\n",
    "        # ----     ----\n",
    "        if episodes_this_epoch > 0:\n",
    "            avg_R = ep_reward_sum / episodes_this_epoch\n",
    "            avg_steps = ep_step_sum / episodes_this_epoch\n",
    "            win_rate = ep_wins / episodes_this_epoch\n",
    "        else:\n",
    "            avg_R = 0.0\n",
    "            avg_steps = 0.0\n",
    "            win_rate = 0.0\n",
    "\n",
    "        learner_idx = league.find_index(\"learner\")\n",
    "        learner_rating = league.players[learner_idx].rating\n",
    "        num_players = len(league.players)\n",
    "\n",
    "        #   \n",
    "        print(\n",
    "            f\"[Epoch {epoch+1}] SUMMARY: \"\n",
    "            f\"W/D/L={ep_wins}/{ep_draws}/{ep_losses} \"\n",
    "            f\"(win_rate={win_rate:.3f}), \"\n",
    "            f\"avg_R={avg_R:.2f}, avg_steps={avg_steps:.2f}, \"\n",
    "            f\"learner_rating={learner_rating:.1f}, \"\n",
    "            f\"num_players={num_players}\"\n",
    "        )\n",
    "\n",
    "        # CSV \n",
    "        append_epoch_log(\n",
    "            epoch=epoch + 1,\n",
    "            episodes=episodes_this_epoch,\n",
    "            wins=ep_wins,\n",
    "            draws=ep_draws,\n",
    "            losses=ep_losses,\n",
    "            avg_reward=avg_R,\n",
    "            avg_steps=avg_steps,\n",
    "            learner_rating=learner_rating,\n",
    "            num_players=num_players,\n",
    "            log_path=log_path,\n",
    "        )\n",
    "\n",
    "        # ---- epoch : PPO  ----\n",
    "        if len(all_states) > 0:\n",
    "            print(f\"[Epoch {epoch+1}] PPO UPDATE: total transitions = {len(all_states)}\")\n",
    "            ppo_update(\n",
    "                policy=learner,\n",
    "                states=all_states,\n",
    "                actions=all_actions,\n",
    "                old_logprobs=all_logprobs,\n",
    "                advantages=all_advantages,\n",
    "                returns=all_returns,\n",
    "                config=config,\n",
    "            )\n",
    "        else:\n",
    "            print(f\"[Epoch {epoch+1}]  transition .\")\n",
    "\n",
    "        # ----   (: 5000 epoch) ----\n",
    "        if (epoch + 1) % checkpoint_interval == 0:\n",
    "            ckpt_path = save_checkpoint_policy(\n",
    "                learner,\n",
    "                epoch=epoch + 1,\n",
    "                checkpoint_dir=checkpoint_dir,\n",
    "                max_keep=max_checkpoints,\n",
    "            )\n",
    "            print(f\"[Epoch {epoch+1}] Checkpoint saved to {ckpt_path}\")\n",
    "\n",
    "        # ---- snapshot  (interval) ----\n",
    "        if (epoch + 1) % snapshot_interval == 0:\n",
    "            learner_idx = league.find_index(\"learner\")\n",
    "            learner_rating = league.players[learner_idx].rating\n",
    "\n",
    "            snap_policy = clone_policy(learner)\n",
    "            snap_id = f\"snap_{snapshot_counter:03d}\"\n",
    "            snapshot_counter += 1\n",
    "\n",
    "            league.players.append(\n",
    "                RatedPolicy(\n",
    "                    id=snap_id,\n",
    "                    policy=snap_policy,\n",
    "                    rating=learner_rating,\n",
    "                    games=0,\n",
    "                )\n",
    "            )\n",
    "            print(f\"[Epoch {epoch+1}] New snapshot added: {snap_id} (rating={learner_rating:.1f})\")\n",
    "\n",
    "    env.close()\n",
    "    return learner, league\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# agent.py   \n",
    "\n",
    "def main_train():\n",
    "    \"\"\"\n",
    "      .\n",
    "    - ELO  self-play learner \n",
    "    -  learner policy 'shared_policy.pt' \n",
    "    \"\"\"\n",
    "    config = PPOConfig(\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_coef=0.2,\n",
    "        ent_coef=0.01,\n",
    "        vf_coef=0.5,\n",
    "        max_grad_norm=0.5,\n",
    "        learning_rate=3e-4,\n",
    "        update_epochs=2,     # /  ,     \n",
    "        batch_size=64,\n",
    "    )\n",
    "\n",
    "    learner, league = train_league_selfplay(\n",
    "        num_epochs=10000,\n",
    "        episodes_per_epoch=50,\n",
    "        snapshot_interval=2,\n",
    "        config=config,\n",
    "        checkpoint_dir=\"checkpoints\",\n",
    "        checkpoint_interval=5000,\n",
    "        max_checkpoints=20,\n",
    "        log_path=\"training_metrics.csv\",\n",
    "    )\n",
    "\n",
    "    #  learner  \n",
    "    weight_path = \"shared_policy.pt\"\n",
    "    learner.save(weight_path)\n",
    "    print(f\"[TRAIN DONE] Saved learner policy to '{weight_path}'\")\n",
    "\n",
    "    #     \n",
    "    print(\"\\n=== Final League Ratings ===\")\n",
    "    for p in league.players:\n",
    "        print(f\"id={p.id}, rating={p.rating:.1f}, games={p.games}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #       \n",
    "    main_train()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc695488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== TEST START ====\n",
      "[OK] preprocessing / encoders\n",
      "[OK] action discretization / network\n",
      "[OK] GAE / terminal reward\n",
      "[OK] PPOPolicy act/save/load/update\n",
      "[OK] ELO / League\n",
      "[OK] env integration (YourBlackAgent / YourWhiteAgent)\n",
      "[Ep 001/002] steps=11, R_black=-1.00, R_white=1.00, transitions_B=6, W=5\n",
      "[Ep 002/002] steps=7, R_black=-1.00, R_white=1.00, transitions_B=4, W=3\n",
      "\n",
      "[ PPO UPDATE ] total transitions = 18\n",
      "\n",
      "===== [Epoch 1/1] =====\n",
      "[Ep 01-001] steps=5, learner_color=1, R_learner=1.00, score=1.0, opp_id=snap_000, learner_rating=1516.0, opp_rating=1484.0\n",
      "[Ep 01-002] steps=7, learner_color=1, R_learner=-1.00, score=0.0, opp_id=snap_000, learner_rating=1498.5, opp_rating=1501.5\n",
      "[Epoch 1] PPO UPDATE: total transitions = 5\n",
      "[Epoch 1] New snapshot added: snap_001 (rating=1498.5)\n",
      "[OK] self-play smoke tests\n",
      "==== ALL TESTS PASSED ====\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "#   (   ipynb  )\n",
    "# ============================================================\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "#  obs \n",
    "# ------------------------------------------------------------\n",
    "def make_dummy_obs(turn: int = 0):\n",
    "    BW, BH = BOARD_W, BOARD_H\n",
    "    black = np.array([\n",
    "        [0.0,      0.0,      1.0],\n",
    "        [BW/2,     BH/2,     1.0],\n",
    "        [BW*1.2,   BH*1.2,   0.0],\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    white = np.array([\n",
    "        [BW*0.8, BH*0.2, 1.0],\n",
    "        [BW*0.9, BH*0.9, 0.0],\n",
    "        [BW*-0.1, BH*1.1, 1.0],\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    obstacles = np.array([\n",
    "        [0.0,     0.0,     BW*0.2,  BH*0.2],\n",
    "        [BW*0.5,  BH*0.3,  BW*0.1,  BH*0.1],\n",
    "        [BW*1.1,  BH*1.1,  BW*0.3,  BH*0.3],\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "    return {\n",
    "        \"black\": black,\n",
    "        \"white\": white,\n",
    "        \"obstacles\": obstacles,\n",
    "        \"turn\": int(turn),\n",
    "    }\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.  / \n",
    "# ------------------------------------------------------------\n",
    "def test_preprocessing():\n",
    "    obs = make_dummy_obs()\n",
    "\n",
    "    # split_me_opp\n",
    "    me0, opp0, obs0, t0 = split_me_opp(obs, my_color=0)\n",
    "    me1, opp1, obs1, t1 = split_me_opp(obs, my_color=1)\n",
    "    assert (me0 == obs[\"black\"]).all()\n",
    "    assert (me1 == obs[\"white\"]).all()\n",
    "    assert t0 == t1 == float(obs[\"turn\"])\n",
    "\n",
    "    # normalize\n",
    "    nst = normalize_stones(obs[\"black\"], BOARD_W, BOARD_H)\n",
    "    assert nst.shape == obs[\"black\"].shape\n",
    "    assert np.all(0 <= nst[:, :2]) and np.all(nst[:, :2] <= 1)\n",
    "\n",
    "    nob = normalize_obstacles(obs[\"obstacles\"], BOARD_W, BOARD_H)\n",
    "    assert nob.shape == obs[\"obstacles\"].shape\n",
    "    assert np.all(0 <= nob) and np.all(nob <= 1)\n",
    "\n",
    "    # encoders\n",
    "    f_basic = encode_state_basic_alkkagi(obs, 0, BOARD_W, BOARD_H)\n",
    "    f_fe = encode_state_fe_alkkagi(obs, 1, BOARD_W, BOARD_H)\n",
    "    assert f_basic.shape == (31,)\n",
    "    assert f_fe.shape == (52,)\n",
    "    assert np.isfinite(f_fe).all()\n",
    "\n",
    "    print(\"[OK] preprocessing / encoders\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. action  / \n",
    "# ------------------------------------------------------------\n",
    "def test_action_and_network():\n",
    "    # action mapping\n",
    "    seen = set()\n",
    "    for idx in range(N_ACTIONS):\n",
    "        s, ang, pw = decode_action_index(idx)\n",
    "        assert 0 <= s < N_STONES\n",
    "        assert -180 <= ang <= 180\n",
    "        assert 500 <= pw <= 2500\n",
    "        seen.add((s, round(ang,4), round(pw,4)))\n",
    "    assert len(seen) == N_ACTIONS\n",
    "\n",
    "    # network forward/backward\n",
    "    net = PolicyValueNet(state_dim=52, n_actions=N_ACTIONS)\n",
    "    x = torch.randn(4, 52)\n",
    "    logits, v = net(x)\n",
    "    assert logits.shape == (4, N_ACTIONS)\n",
    "    assert v.shape == (4,)\n",
    "    (logits.mean() + v.mean()).backward()\n",
    "    assert any(p.grad is not None for p in net.parameters())\n",
    "\n",
    "    print(\"[OK] action discretization / network\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. GAE, terminal reward\n",
    "# ------------------------------------------------------------\n",
    "def test_gae_reward():\n",
    "    obs = make_dummy_obs()\n",
    "    obs[\"black\"][:,2] = [1,1,0]\n",
    "    obs[\"white\"][:,2] = [1,0,1]\n",
    "    r0 = compute_terminal_reward(obs, 0)\n",
    "    r1 = compute_terminal_reward(obs, 1)\n",
    "    assert abs(r0 + r1) < 1e-6\n",
    "\n",
    "    # GAE\n",
    "    rewards = np.array([0,0,1], np.float32)\n",
    "    values = np.zeros(3, np.float32)\n",
    "    adv, ret = compute_gae_returns(rewards, values, 0.99, 0.95)\n",
    "    assert adv.shape == (3,)\n",
    "    assert ret.shape == (3,)\n",
    "\n",
    "    print(\"[OK] GAE / terminal reward\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. PPOPolicy act/save/load/update\n",
    "# ------------------------------------------------------------\n",
    "def test_policy_functions():\n",
    "    device = torch.device(\"cpu\")\n",
    "    policy = PPOPolicy(device=device)\n",
    "\n",
    "    obs = make_dummy_obs(0)\n",
    "    act = policy.act_eval(obs, 0)\n",
    "    assert isinstance(act, dict)\n",
    "\n",
    "    a, aidx, lp, val, st = policy.act_train(obs, 0)\n",
    "    assert isinstance(aidx, int)\n",
    "    assert st.shape == (52,)\n",
    "\n",
    "    # save/load\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".pt\", delete=False) as f:\n",
    "        path = f.name\n",
    "    policy.save(path)\n",
    "    pol2 = PPOPolicy.load(path, device=device)\n",
    "    os.remove(path)\n",
    "\n",
    "    for k,v in policy.model.state_dict().items():\n",
    "        assert torch.allclose(v, pol2.model.state_dict()[k])\n",
    "\n",
    "    # ppo_update    \n",
    "    states = [torch.randn(52) for _ in range(32)]\n",
    "    actions = [np.random.randint(0, N_ACTIONS) for _ in range(32)]\n",
    "    old = [0.0]*32\n",
    "    adv = np.random.randn(32).tolist()\n",
    "    ret = np.random.randn(32).tolist()\n",
    "\n",
    "    before = {k:v.clone() for k,v in policy.model.state_dict().items()}\n",
    "    cfg = PPOConfig(update_epochs=1, batch_size=16)\n",
    "    ppo_update(policy, states, actions, old, adv, ret, cfg)\n",
    "    after = policy.model.state_dict()\n",
    "\n",
    "    changed = any(not torch.allclose(before[k], after[k]) for k in before)\n",
    "    assert changed\n",
    "\n",
    "    print(\"[OK] PPOPolicy act/save/load/update\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. ELO / \n",
    "# ------------------------------------------------------------\n",
    "def test_elo_league():\n",
    "    p1 = RatedPolicy(id=\"a\", policy=PPOPolicy(), rating=1500)\n",
    "    p2 = RatedPolicy(id=\"b\", policy=PPOPolicy(), rating=1400)\n",
    "    league = EloLeague([p1,p2])\n",
    "\n",
    "    opp = league.choose_opponent(\"a\")\n",
    "    assert opp.id == \"b\"\n",
    "\n",
    "    league.update_result(\"a\",\"b\",1.0)\n",
    "    assert p1.rating > 1500\n",
    "    assert p2.rating < 1400\n",
    "\n",
    "    print(\"[OK] ELO / League\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. kymnasium env  \n",
    "# ------------------------------------------------------------\n",
    "def test_env_integration():\n",
    "    env = make_env(render_mode=None, bgm=False)\n",
    "    device = torch.device(\"cpu\")\n",
    "    policy = PPOPolicy(device=device)\n",
    "    black = YourBlackAgent(policy, device)\n",
    "    white = YourWhiteAgent(policy, device)\n",
    "\n",
    "    obs, info = env.reset(seed=0)\n",
    "    for _ in range(10):\n",
    "        if obs[\"turn\"] == 0:\n",
    "            action = black.act(obs, info)\n",
    "        else:\n",
    "            action = white.act(obs, info)\n",
    "\n",
    "        obs, rew, term, trunc, info = env.step(action)\n",
    "        if term or trunc:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    print(\"[OK] env integration (YourBlackAgent / YourWhiteAgent)\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. self-play  smoke test\n",
    "# ------------------------------------------------------------\n",
    "def test_selfplay_smoke():\n",
    "    cfg = PPOConfig(update_epochs=1, batch_size=32)\n",
    "\n",
    "    # shared PPO trainer\n",
    "    try:\n",
    "        p = train_selfplay_shared_ppo(num_episodes=2, config=cfg)\n",
    "        assert isinstance(p, PPOPolicy)\n",
    "    except Exception as e:\n",
    "        print(\"[FAIL] selfplay_shared_ppo:\", e)\n",
    "        raise\n",
    "\n",
    "    # league trainer\n",
    "    try:\n",
    "        learner, league = train_league_selfplay(\n",
    "            num_epochs=1, episodes_per_epoch=2, snapshot_interval=1, config=cfg\n",
    "        )\n",
    "        assert isinstance(learner, PPOPolicy)\n",
    "        assert isinstance(league, EloLeague)\n",
    "    except Exception as e:\n",
    "        print(\"[FAIL] league_selfplay:\", e)\n",
    "        raise\n",
    "\n",
    "    print(\"[OK] self-play smoke tests\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "#  \n",
    "# ============================================================\n",
    "print(\"==== TEST START ====\")\n",
    "test_preprocessing()\n",
    "test_action_and_network()\n",
    "test_gae_reward()\n",
    "test_policy_functions()\n",
    "test_elo_league()\n",
    "test_env_integration()\n",
    "test_selfplay_smoke()\n",
    "print(\"==== ALL TESTS PASSED ====\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d175a9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EVAL] episode finished in 6 steps\n"
     ]
    }
   ],
   "source": [
    "# evaluate.py\n",
    "import gymnasium as gym\n",
    "import kymnasium as kym  # env \n",
    "\n",
    "# from agent import YourBlackAgent, YourWhiteAgent\n",
    "\n",
    "\n",
    "def main(render_mode: str = \"human\"):\n",
    "    env = gym.make(\n",
    "        id=\"kymnasium/AlKkaGi-3x3-v0\",\n",
    "        render_mode=render_mode,\n",
    "        obs_type=\"custom\",\n",
    "        bgm=True,\n",
    "    )\n",
    "\n",
    "    #   agent.py  weight \n",
    "    weight_path = \"shared_policy.pt\"\n",
    "\n",
    "    agent_black = YourBlackAgent.load(weight_path)\n",
    "    agent_white = YourWhiteAgent.load(weight_path)\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "\n",
    "    while not done:\n",
    "        if obs[\"turn\"] == 0:\n",
    "            action = agent_black.act(obs, info)\n",
    "        else:\n",
    "            action = agent_white.act(obs, info)\n",
    "\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        step += 1\n",
    "\n",
    "    env.close()\n",
    "    print(f\"[EVAL] episode finished in {step} steps\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(render_mode=\"human\")  #    None  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6716f46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate.py\n",
    "import gymnasium as gym\n",
    "import kymnasium as kym  # env \n",
    "\n",
    "def main():\n",
    "    env = gym.make(\n",
    "        id=\"kymnasium/AlKkaGi-3x3-v0\",\n",
    "        render_mode=\"human\",   #   None\n",
    "        obs_type=\"custom\",\n",
    "        bgm=True,\n",
    "    )\n",
    "\n",
    "    #       \n",
    "    weight_path_black = \"shared_policy.pt\"\n",
    "    weight_path_white = \"shared_policy.pt\"\n",
    "\n",
    "    agent_black = YourBlackAgent.load(weight_path_black)\n",
    "    agent_white = YourWhiteAgent.load(weight_path_white)\n",
    "\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        if obs[\"turn\"] == 0:\n",
    "            action = agent_black.act(obs, info)\n",
    "        else:\n",
    "            action = agent_white.act(obs, info)\n",
    "\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
